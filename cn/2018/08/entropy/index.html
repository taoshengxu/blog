<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Entropy、 交叉熵、相对熵、巴氏距离（Bhattacharyya distance） - My Lives</title>
    <meta property="og:title" content="Entropy、 交叉熵、相对熵、巴氏距离（Bhattacharyya distance） - My Lives">
    

    
      
    

    

    
    

    <link rel="stylesheet" href="/blog/css/style.css" />
    <link rel="stylesheet" href="/blog/css/fonts.css" />
    <link rel="stylesheet" href="/blog/css/custom.css" />

  </head>

  
  <body class="blog">
    <header class="masthead">
      <h1><a href="/blog/">My Lives</a></h1>



      <nav class="menu">
        <input id="menu-check" type="checkbox" />
        <label id="menu-label" for="menu-check" class="unselectable">
          <span class="icon close-icon">✕</span>
          <span class="icon open-icon">☰</span>
          <span class="text">Menu</span>
        </label>
        <ul>
        
        
        <li><a href="/blog/">Home</a></li>
        
        <li><a href="/blog/en/">English</a></li>
        
        <li><a href="/blog/cn/">Chinese</a></li>
        
        <li><a href="/blog/paper/">Paper Reading</a></li>
        
        <li><a href="/blog/travel/">Travel</a></li>
        
        
        </ul>
      </nav>
    </header>

    <article class="main">
      <header class="title">
      
<h1>Entropy、 交叉熵、相对熵、巴氏距离（Bhattacharyya distance）</h1>

<h3>
  2018-08-26</h3>
<hr>


      </header>





<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p><a href="https://blog.csdn.net/u013829973/article/details/80936272">原文1</a> <a href="https://www.jianshu.com/p/43318a3dc715?from=timeline&amp;isappinstalled=0">原文2</a> <a href="http://blog.sina.com.cn/s/blog_85f1ffb70101e6p1.html">原文3</a></p>

<h2 id="信息量-自信息">信息量(自信息)</h2>

<p>定义：假设X是一个离散型随机变量，其取值集合为χ，概率分布函数为p(x)=P(X=x),x∈χ,我们定义事件X=x0的自信息为：
$$
I(x0)=−log(p(x0))
$$</p>

<p>一个事件发生的概率越大，则它所携带的信息量就越小，而当p(x0)=1时，熵将等于0，也就是说该事件的发生包含的信息量小。</p>

<p>举个例子，小明平时不爱学习，考试经常不及格，而小王是个勤奋学习的好学生，经常得满分，所以我们可以做如下假设：
事件A：小明考试及格，对应的概率P(xA)=0.1，信息量为I(xA)=−log(0.1)=3.3219.
事件B：小王考试及格，对应的概率P(xB)=0.999，信息量为I(xB)=−log(0.999)=0.0014</p>

<p>可以看出，结果非常符合直观：小明及格的可能性很低(十次考试只有一次及格)，因此如果某次考试及格了（大家都会说：XXX竟然及格了！），必然会引入较大的信息量，对应的I值也较高。而对于小王而言，考试及格是大概率事件，在事件B发生前，大家普遍认为事件B的发生几乎是确定的，因此当某次考试小王及格这个事件发生时并不会引入太多的信息量，相应的I值也非常的低。</p>

<h4 id="5-mutual-information-互信息-mi-normalized-mutual-information-标准互信息-nmi">5、Mutual Information(互信息)(MI)，Normalized Mutual Information (标准互信息)(NMI)</h4>

<p><strong>互信息（Mutual Information）</strong>是用来衡量两个数据分布的吻合程度，是指两个事件集合之间的相关性。</p>

<p><code>特征选择：</code>用互信息的方法，在某个类别C中的出现概率高，而在其它类别中的出现概率低的词条T，将获得较高的词条和类别互信息，也就可能被选取为类别C的特征。互信息是term的存在与否能给类别c的正确判断带来的信息量。词条和类别的互信息体现了词条和类别的相关程度，互信息越大，词条和类别的相关程度也越大。得到词条和类别之间的相关程度后，选取一定比例的，排名靠前的词条作为最能代表此种类别的特征。</p>

<p><img src="https://taoshengxu.github.io/DocumentGit/img/cluter_criteria9.png" alt="" /></p>

<p>标准化互聚类信息都是用熵做分母将MI值调整到0与1之间，一个比较多见的实现是下面所示：</p>

<p><img src="https://taoshengxu.github.io/DocumentGit/img/cluter_criteria10.png" alt="" /></p>

<h3 id="例子-假设对于17个样本点-v1-v2-v17-进行聚类">例子：假设对于17个样本点(v1,v2,&hellip;,v17)进行聚类</h3>

<p><img src="http://images.cnblogs.com/cnblogs_com/ziqiao/stanford%E8%81%9A%E7%B1%BB%E4%BE%8B%E5%AD%90.jpg" alt="img" /></p>

<p>比如标准结果是图中的叉叉点点圈圈，我的聚类结果是图中标注的三个圈。</p>

<blockquote>
<p>或者我的结果: A = [1 1 1 1 1 1   2 2 2 2 2 2    3 3 3 3 3];</p>

<p>标准的结果   : B = [1 2 1 1 1 1   1 2 2 2 2 3    1 1 3 3 3];</p>
</blockquote>

<p><code>问题：</code>衡量我的结果和标准结果有多大的区别，若我的结果和他的差不多，结果应该为1，若我做出来的结果很差，结果应趋近于0。
$$
MI(X,Y)=\sum<em>{i=1}^{|X|}\sum</em>{j=1}^{|Y|}P(i,j)log(\frac{P(i,j)}{P(i)P^{&lsquo;}(j)})
$$
首先计算上式分子中联合概率分布 \(P(i,j)=\frac{|X<em>i\cap Y</em>j|}{N}\)</p>

<p>X=unique(A)=[1 2 3]，Y=unique(B)=[1 2 3];</p>

<p>分子p(x,y)为x和y的联合分布概率，</p>

<p>p(1,1)=<sup>5</sup>&frasl;<sub>17</sub>, p(1,2)=<sup>1</sup>&frasl;<sub>17</sub>, p(1,3)=0;</p>

<p>p(2,1)=<sup>1</sup>&frasl;<sub>17</sub>, p(2,2)=<sup>4</sup>&frasl;<sub>17</sub>, p(2,3)=<sup>1</sup>&frasl;<sub>17</sub>;</p>

<p>p(3,1)=<sup>2</sup>&frasl;<sub>17</sub>, p(3,2)=0, p(3,3)=<sup>3</sup>&frasl;<sub>17</sub>;</p>

<p>分母p(x)为x的概率函数，p(y)为y的概率函数，x和y分别来自于A和B中的分布，所以即使x=y时，p(x)和p(y)也可能是不一样的。</p>

<p>对p(x)： p(1)=<sup>6</sup>&frasl;<sub>17</sub> p(2)=<sup>6</sup>&frasl;<sub>17</sub> p(3)=<sup>5</sup>&frasl;<sub>17</sub></p>

<p>对p(y)： p(1)=<sup>8</sup>&frasl;<sub>17</sub> p(2)=<sup>5</sup>&frasl;<sub>17</sub> P(3)=<sup>4</sup>&frasl;<sub>17</sub></p>

<p>这样就可以算出MI值了。</p>

<p><strong>互信息</strong>
$$
NMI(X,Y)=\frac{2MI(X,Y)}{H(X)+H(Y)}
$$
H(X)和H(Y)分别为X和Y的熵，下面的公式中log的底b=2。</p>

<p><img src="http://images.cnblogs.com/cnblogs_com/ziqiao/shang.png" alt="img" /></p>

<p>例如H(X) =  -p(1)<em>log2(p(1)) - -p(2)</em>log2(p(2)) -p(3)*log2(p(3))。</p>

<h2 id="熵-entropy">熵（Entropy）</h2>

<p>定义：对于一个随机变量XX而言，它的所有可能取值的信息量的期望\(（E[I(x)])就称为熵。
当XX是离散的：
$$
H(X)=E[I(x)]=−∑_{x∈X}p(x)logp(x)
$$</p>

<p>当X是连续的随机变量：熵定义为：
$$
H(X)=−∫_{x∈X}p(x)logp(x)dx
$$
<strong>自信息只能处理单个的输出。而熵是对整个概率分布中不确定性总量进行量化</strong>。</p>

<p>约定：p=0时，定义0log0=0。通常对数以2为底或者e为底，这是熵的单位称作比特（bit）或者纳特（nat）
当随机变量只取两个值的时候，即X分布为：P(X=1)=p，P(X=0)=1−p，0&lt;=p&lt;=1。</p>

<p>还是通过上边的例子来说明，假设小明的考试结果是一个0-1分布X只有两个取值{0：不及格，1：及格}，在某次考试结果公布前，小明的考试结果有多大的不确定度呢？你肯定会说：十有八九不及格！因为根据先验知识，小明及格的概率仅有0.1,90%的可能都是不及格的。怎么来度量这个不确定度？求期望！不错，我们对所有可能结果带来的额外信息量求取均值（期望），其结果不就能够衡量出小明考试成绩的不确定度了吗。
即：
HA(x)=−[p(xA)log(p(xA))+(1−p(xA))log(1−p(xA))]=0.4690
对应小王的熵：
HB(x)=−[p(xB)log(p(xB))+(1−p(xB))log(1−p(xB))]=0.0114
虽然小明考试结果的不确定性较低，毕竟十次有9次都不及格，但是也比不上小王（1000次考试只有一次才可能不及格，结果相当的确定）
我们再假设一个成绩相对普通的学生小东，他及格的概率是P(xC)=0.5,即及格与否的概率是一样的，对应的熵：
HC(x)=−[p(xC)log(p(xC))+(1−p(xC))log(1−p(xC))]=1
其熵为1，他的不确定性比前边两位同学要高很多，在成绩公布之前，很难准确猜测出他的考试结果。
可以看出，<strong>熵其实是信息量的期望值，它是一个随机变量的不确定性的度量。熵越大，随机变量的不确定性越大</strong>。</p>

<h2 id="相对熵-kl-kullback-leibler-divergence-散度">相对熵&ndash;KL（Kullback-Leibler divergence）散度</h2>

<p><strong>描述两个概率分布P和Q差异的一种方法,在信息论中，D(P||Q)表示当用概率分布Q来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。相对熵可以衡量两个随机分布之间的距离，当两个随机分布相同时，它们的相对熵为零，当两个随机分布的差别增大时，它们的相对熵也会增大。所以相对熵（KL散度）可以用于比较文本的相似度，先统计出词的频率，然后计算相对熵。</strong></p>

<p>KL距离，是对同一个随机变量X的两个单独的概率分布的度量。记为\(D_{KL}(p||q)\)。 它度量当真实分布为p时，假设分布q的无效性。</p>

<p>$$
D<em>{KL}(p||q)=Ep[logp(x)q(x)]=∑</em>{x∈χ}p(x)logp(x)q(x)  <br />
=∑<em>{x∈χ}[p(x)logp(x)−p(x)logq(x)] <br />
=∑</em>{x∈χ}p(x)logp(x)−∑<em>{x∈χ}p(x)logq(x) <br />
=−H(p)−∑</em>{x∈χ}p(x)logq(x) <br />
=−H(p)+Ep[−logq(x)] <br />
=Hp(q)−H(p)
$$</p>

<p>并且为了保证连续性，做如下约定：
0log0/0=0，0log0/q=0，plogp/0=∞
显然，当p=q时,两者之间的相对熵DKL(p||q)=0
上式最后的Hp(q)表示在p分布下，使用q进行编码需要的bit数，而H(p)表示对真实分布p所需要的最小编码bit数。基于此，相对熵的意义就很明确了：DKL(p||q)表示在真实分布为p的前提下，使用q分布进行编码相对于使用真实分布p进行编码（即最优编码）所多出来的bit数。</p>

<p><strong>重要性质 ：</strong></p>

<p><strong>1.它是非负的。</strong></p>

<p><strong>2.不是对称的:对于某些P和Q，DKL(p||q)DKL(p||q)不等于DKL(q||p)DKL(q||p),这样意味着选择DKL(p||q)DKL(p||q)还是DKL(q||p)DKL(q||p)影响很大</strong></p>

<p>例子</p>

<p>假如一个字符发射器，随机发出0和1两种字符，真实发出概率分布为A，但实际不知道A的具体分布。通过观察，得到概率分布B与C，各个分布的具体情况如下：</p>

<p><img src="https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D160/sign=7c3a2d55bb7eca8016053de1a1239712/95eef01f3a292df52ed52763b7315c6034a873b6.jpg" alt="img" /></p>

<p><img src="https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D156/sign=c23f2c659cdda144de0968b784b6d009/8d5494eef01f3a29e31d5db89225bc315d607ceb.jpg" alt="img" /></p>

<p><img src="https://gss0.bdstatic.com/-4o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D158/sign=6956084631f33a879a6d041ffe5d1018/2e2eb9389b504fc2690d3b73eedde71190ef6d72.jpg" alt="img" /></p>

<p>可以计算出得到如下：</p>

<p><img src="https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D366/sign=2c3e1328d6c451daf2f60aed80fc52a5/b64543a98226cffc29fc34c4b2014a90f703eaf6.jpg" alt="img" /></p>

<p><img src="https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D378/sign=b52422c4da58ccbf1fbcb33d21d9bcd4/fd039245d688d43f816e1a84761ed21b0ff43b84.jpg" alt="img" /></p>

<p>也可以看出，按照概率分布B进行编码，要比按照C进行编码，平均每个符号增加的比特数目少。从分布上也可以看出，实际上B比C更接近实际分布（因为其与A分布的相对熵更小）.</p>

<h2 id="交叉熵-cross-entropy">交叉熵(Cross-Entropy)</h2>

<p>交叉熵容易跟相对熵搞混，二者联系紧密，但又有所区别。假设有两个分布p，q则它们在给定样本集上的交叉熵定义如下：
$$
CEH(p,q)=Ep[−logq]=−∑x∈χp(x)logq(x)=H(p)+DKL(p||q)
$$</p>

<p>可以看出，交叉熵与上一节定义的相对熵仅相差了H(p),当p已知时，可以把H(p)看做一个常数，此时交叉熵与KL距离在行为上是等价的，都反映了分布p、q的相似程度。最小化交叉熵等于最小化KL距离。它们都将在p=q时取得最小值H(p)（p=q时KL距离为0），因此有的工程文献中将最小化KL距离的方法称为Principle of Minimum Cross-Entropy (MCE)或Minxent方法。</p>

<h4 id="由交叉熵到logistic-regression">由交叉熵到logistic regression</h4>

<p><strong>特别的，在logistic regression中</strong>
p:真实样本分布，服从参数为p的0-1分布，即X∼B(1,p)
q:待估计的模型，服从参数为q的0-1分布，即X∼B(1,q)
两者的交叉熵为：
$$
CEH(p,q)   <br />
=−∑x∈χp(x)logq(x)  <br />
=−[Pp(x=1)logPq(x=1)+Pp(x=0)logPq(x=0)] <br />
=−[plogq+(1−p)log(1−q)] \<br />
=−[yloghθ(x)+(1−y)log(1−hθ(x))]<br />
$$</p>

<p>对所有训练样本取均值得：
$$
−(1/m)∑^m_{i=1}[y(i)loghθ(x(i))+(1−y(i))log(1−hθ(x(i)))]
$$
这个结果<strong>与通过极大似然估计方法求出来的结果是一致的；最小化交叉熵损失函数等价于求极大似然估计；从二者的公式来看，只是差了一个负号而已。</strong></p>

<p><strong>那么LR的损失函数为什么不用平方损失呢？因为平方损失函数不是凸函数，使用梯度下降法无法求得局部最小（全局最小），而交叉熵损失函数是凸函数，使用梯度下降法可以找到全局最优解。</strong></p>

<h2 id="bhattacharyya-distance巴氏距离">Bhattacharyya distance巴氏距离</h2>

<ul>
<li>在统计理论中，<strong>Bhattacharyya距离用来度量两个离散或连续概率分布的相似性</strong>。</li>
<li>它与Bhattacharyya系数（Bhattacharyya coefficient）高度相关，后者是用来度量两个统计样本的重叠度的。所有这些命名都是为了纪念A. Bhattacharyya，一个在1930年工作于印度统计局的统计学家</li>
<li><strong>该系数可以用来度量两个样本集的相似性。它通常在分类问题中被用来判断类别的可分性。</strong></li>
</ul>

<p>定义</p>

<p>对于定义在同一个定义域X上的两个离散概率分布p和q来说，它们之间的Bhattacharyya距离可定义如下：</p>

<p><img src="http://upload.wikimedia.org/math/b/a/d/badf93af7fd3dce1978276df77bf3264.png" alt="D_B(p,q) = -\ln \left( BC(p,q) \right)" /></p>

<p>这里</p>

<p><img src="http://upload.wikimedia.org/math/b/3/0/b30f018b6017164a220084802c4417cc.png" alt="BC(p,q) = \sum_{x\in X} \sqrt{p(x) q(x)}" /></p>

<p>被称为Bhattacharyya系数。</p>

<p>对于连续概率分布，Bhattacharyya系数可以定义如下：</p>

<p><img src="http://upload.wikimedia.org/math/7/1/6/716ecb5f3bcb9b3256e713b9149cdc70.png" alt="BC(p,q) = \int \sqrt{p(x) q(x)}\, dx" /></p>

<p>在以上两种情况下，0&lt;=BC&lt;=1并且0&lt;=DB&lt;=∞。DB并不遵循三角不等式，但是<a href="http://blog.sina.com.cn/s/blog_85f1ffb70101e65d.html">Hellinger距离</a>满足三角不等式。</p>

<p>对于一个多维高斯分布来说pi=N(mi,Pi)，</p>

<p><img src="http://upload.wikimedia.org/math/5/3/4/5343dbab9715adc082917563efac53d3.png" alt="D_B={1\over 8}(m_1-m_2)^T P^{-1}(m_1-m_2)+{1\over 2}\ln \,\left({\det P \over \sqrt{\det P_1 \, \det P_2} }\right)" /></p>

<p>这里mi和Pi分别代表该分布的均值和方差，并且</p>

<p><img src="http://upload.wikimedia.org/math/4/6/d/46dd37a40d8bcd7cd90824e9308e210d.png" alt="P={P_1+P_2 \over 2}" /></p>

<p>注意到，在这种情况下Bhattacharyya距离的第一项类似于Mahalanobis距离（马氏距离）。</p>

<h4 id="bhattacharyya系数">Bhattacharyya系数</h4>

<p>Bhattacharyya系数用来度量两个统计样本的重叠度。该系数可以用来度量两个样本集的可分性。</p>

<p>计算Bhattacharyya系数包含了一个基本的关于两个样本集重合度的积分运算。两个样本集中的定义域被分成了事前定义的几份，这种划分可以体现在下面的定义中：</p>

<p><img src="http://upload.wikimedia.org/math/0/7/8/07864c05ee66eeaa6eec0c528f94ab03.png" alt="\mathrm{Bhattacharyya} = \sum_{i=1}^{n}\sqrt{(\mathbf{\Sigma a}_i\cdot\mathbf{\Sigma b}_i)}" /></p>

<p>其中a，b代表样本，n代表划分的数目，∑ai和∑bi分别代表两个样本集中在第i个划分中的样本之和。</p>

<p>对于两个样本集来说，如果相同划分中的样本数越多，样本和越大，则该式的值越大。划分数的选择取决于每一个样本集中的样本数：太少的划分将因为过高估计了重叠区域而减小精度，而太多的划分将会因为在本该有重叠的区域没有恰好重叠而减小精度（最精细的划分将会使每一个相同的区间中都没有重叠）。</p>

<p>如果在每一个划分区间内的乘积都为零，则Bhattacharyya系数也为零。这就意味着如果A和B两个样本集都与样本集C完全可分，则BC（A，C）=B（B，C）=0，即Bhattacharyya系数对于A和B无法区分。</p>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/blog/cn/2018/06/gap_statistic/">Gap Statistic 间隔统计量</a></span>
  <span class="nav-next"><a href="/blog/cn/2018/08/machinelearing_rpackages/">R语言机器学习包</a> &rarr;</span>
</nav>





<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  

  
  <hr>
  <div class="copyright">&copy; <a href="xxxx">Who care</a> 2017 - Forever, maybe</div>
  
  </footer>
  </article>
  
  </body>
</html>


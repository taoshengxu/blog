<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>岭回归-Lasso-多重共线性 - My Lives</title>
    <meta property="og:title" content="岭回归-Lasso-多重共线性 - My Lives">
    

    
      
    

    

    
    


<link href='//cdn.bootcss.com/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



    <link rel="stylesheet" href="/blog/css/style.css" />
    <link rel="stylesheet" href="/blog/css/fonts.css" />
    <link rel="stylesheet" href="/blog/css/custom.css" />

  </head>

  
  <body class="blog">
    <header class="masthead">
      <h1><a href="/blog/">My Lives</a></h1>



      <nav class="menu">
        <input id="menu-check" type="checkbox" />
        <label id="menu-label" for="menu-check" class="unselectable">
          <span class="icon close-icon">✕</span>
          <span class="icon open-icon">☰</span>
          <span class="text">Menu</span>
        </label>
        <ul>
        
        
        <li><a href="/blog/">Home</a></li>
        
        <li><a href="/blog/en/">English</a></li>
        
        <li><a href="/blog/cn/">Chinese</a></li>
        
        <li><a href="/blog/travel/">Travel</a></li>
        
        
        </ul>
      </nav>
    </header>

    <article class="main">
      <header class="title">
      
<h1>岭回归-Lasso-多重共线性</h1>

<h3>
  2017-09-22</h3>
<hr>


      </header>





<p><a href="http://f.dataguru.cn/thread-598486-1-1.html">原文</a></p>

<h1 id="1-回归问题的数学描述">1. 回归问题的数学描述</h1>

<p>1.n个样本，p个变量，X，y已知。对数据中心化、标准化处理后，可以去掉截距项。
<img src="http://attachbak.dataguru.cn/attachments/forum/201602/29/145150y9utqyr9p0s99pk0.jpg" alt="" /></p>

<p>2.矩阵形式的多元线性模型为:</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201602/29/145822oknvx60lzl92v691.png" alt="" /></p>

<p>求解β，使得误差项ε能达到较低.</p>

<p>3.残差平方和RSS为</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201602/29/151838z32fr356n52in5m3.png" alt="" /></p>

<p>4.多元线性回归问题变为求解β，从而使残差平方和极小值问题（关于系数向量β的二次函数极值问题）</p>

<p>5.几何意义</p>

<p>残差向量的几何意义：响应y向量到由p个x向量组成的超平面的距离向量。<br>
残差平方和几何意义：残差向量长度的平方。</p>

<h1 id="2-最小二乘回归">2.最小二乘回归</h1>

<p>β的最小二乘估计为：</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201602/29/170846uzhmw4wrcvwhkazy.jpg" alt="" /></p>

<p>在统计学上，可证明β的最小二乘解为无偏估计，即多次得到的采样值X而计算出来的多个系数估计值向量的平均值将无限接近于真实值向量β。</p>

<p>如果存在较强的共线性，即X中各列向量之间存在较强的相关性，会导致|X^T X|≈0, 从而引起对角线上的值很大(X^T X的逆矩阵不不存在)</p>

<hr />

<p>问题： X矩阵不存在广义逆（即奇异性）的情况。</p>

<ul>
<li><p>X本身存在线性相关关系（即多重共线性），即非满秩矩阵。当采样值误差造成本身线性相关的样本矩阵仍然可以求出逆阵时，此时的逆阵非常不稳定，所求的解也没有什么意义。</p></li>

<li><p>当变量比样本多，即p&gt;n时.回归系数会变得很大，无法求解。</p></li>
</ul>

<h1 id="3-岭回归-ridge-regression-rr-1962">3.岭回归（Ridge Regression，RR, 1962）</h1>

<p>思路：在原先的β的最小二乘估计中加一个小扰动λI，是原先无法求广义逆的情况变成可以求出其广义逆，使得问题稳定并得以求解。</p>

<p>极值问题：
<img src="http://attachbak.dataguru.cn/attachments/forum/201603/01/083631kd5q6mjtdjwyuy15.jpg" alt="" />
<img src="http://attachbak.dataguru.cn/attachments/forum/201603/01/083802zffpx1zf6mlzf3ax.jpg" alt="" /></p>

<p>对上式用偏导数求极值，结果就是</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/01/160953w8k63tvoa76wpw18.jpg" alt="" /></p>

<p>其中<img src="http://attachbak.dataguru.cn/attachments/forum/201603/01/083451yxgvv2oacxvaaau4.jpg" alt="" />为惩罚函数，它保证了β值不会变的很大。岭参数λ不同，岭回归系数也会不同。</p>

<p>岭回归是回归参数β的有偏估计。它的结果是使得残差平和变大，但是会使系数检验变好，即R语言summary结果中变量后的*变多。</p>

<p>岭回归缺陷:</p>

<ul>
<li><p>1.主要靠目测选择岭参数</p></li>

<li><p>2.计算岭参数时，各种方法结果差异较大</p></li>
</ul>

<p>所以一般认为，岭迹图只能看多重共线性，却很难做变量筛选</p>

<h1 id="4-几何解释">4.几何解释</h1>

<p>以两个变量为例，系数β1和β2已经经过标准化。残差平方和RSS可以表示为β1和β2的一个二次函数，数学上可以用一个抛物面表示。</p>

<ol>
<li>最小二乘法</li>
</ol>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/01/153022dvegnfni1s9d9eov.jpg" alt="" /></p>

<p>2.岭回归</p>

<p>约束项为 β1^2+β2^2≤t</p>

<p>对应着投影为β1和β2平面上的一个圆，即下图中的圆柱.</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/01/154311l7qrvvmtq93grvmw.jpg" alt="" /></p>

<p>该圆柱与抛物面的交点对应的β1、β2值，即为满足约束项条件下的能取得的最小的β1和β2.</p>

<p>从β1,β2平面理解，即为抛物面等高线在水平面的投影和圆的交点，如下图所示,可见岭回归解与原先的最小二乘解是有一定距离的。</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/03/110921ybbab0oq9tbn9qpf.jpg" alt="" /></p>

<p>3.岭回归性质</p>

<p><img src="https://taoshengxu.github.io/DocumentGit/img/Ridge20170922104329.png" alt="" /></p>

<p>4.岭迹图</p>

<p>岭迹图作用：</p>

<ul>
<li><p>1）观察λ较佳取值；</p></li>

<li><p>2）观察变量是否有多重共线性；</p></li>
</ul>

<p>是λ的函数，岭迹图的横坐标为λ，纵坐标为β(λ)。而β(λ)是一个向量，由β1(λ)、β2(λ)、&hellip;等很多分量组成，每一个分量都是λ的函数，将每一个分量分别用一条线。当不存在奇异性时，岭迹应是稳定地逐渐趋向于0。
<img src="http://attachbak.dataguru.cn/attachments/forum/201603/04/173356ws06xw7xqov8wc0o.jpg" alt="" /></p>

<p>岭迹图比较</p>

<p><img src="https://taoshengxu.github.io/DocumentGit/img/Ridge20170922154100.png" alt="" /></p>

<p>通过岭迹的形状来判断我们是否要剔除掉该参数（例如：岭迹波动很大，说明该变量参数有共线性）</p>

<p>可见，在λ很小时，通常各β系数取值较大；而如果λ=0，则跟普通意义的多元线性回归的最小二乘解完全一样；当λ略有增大，则各β系数取值迅速减小，即从不稳定趋于稳定。上图类似喇叭形状的岭迹图，一般都存在多重共线性。</p>

<ul>
<li><p>λ的选择：一般通过观察，选取喇叭口附近的值，此时各β值已趋于稳定，但总的RSS又不是很大。</p></li>

<li><p>选择变量：删除那些β取值一直趋于0的变量。</p></li>

<li><p>注意：用岭迹图筛选变量并非十分靠谱。</p></li>
</ul>

<p>岭回归选择变量的原则（不靠谱，仅供参考）
* 1）在岭回归中设计矩阵X已经中心化和标准化了，这样可以直接比较标准化岭回归系数癿大小。可以剔除掉标准化岭回归系数比较稳定且值很小癿自变量。</p>

<ul>
<li><p>2）随着λ的增加，回归系数不稳定，震动趋于零的自变量也可以剔除。</p></li>

<li><p>3）如果依照上述去掉变量的原则，有若干个回归系数不稳定，究竟去掉几个，去掉哪几个，这幵无一般原则可循，这需根据去掉某个变量后重新进行岭回归分析的效果来确定。</p></li>
</ul>

<p>5.岭回归R语言分析</p>

<pre><code class="language-R">library(MASS)#岭回归在MASS包中。
longley #内置数据集，有关国民经济情况的数据，以多重共线性较强著称
summary(fm1&lt;-lm(Employed~.,data=longley)) #最小二乘估计的多元线性回归
#结果可见，R^2很高，但是系数检验不是非常理想
names(longley)[1]&lt;-&quot;y&quot;  
lm.ridge(y~.,longley)   #此时，仍为线性回归
plot(lm.ridge(y~.,longley,lambda=seq(0,0.1,0.001)))  #加了参数lambda的描述后才画出响应的岭迹图
#由于lambda趋于0时，出现了不稳定的情况，所以可以断定变量中存在多重共线性
select(lm.ridge(y~.,longley,lambda=seq(0,0.1,0.001)))  #用select函数可算lambda值，结果给出了3种方法算的的lambda的估计值

## modified HKB estimator is 0.006836982 
## modified L-W estimator is 0.05267247 
## smallest value of GCV  at 0.006 

#以上结果通常取GCV估计，或者观察大多数方法趋近哪个值。
</code></pre>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/04/180759vkgxxkvjtvkozvvx.jpg" alt="" /></p>

<h1 id="5-lasso">5. LASSO</h1>

<p>Tibshirani(1996)提出了Lasso(The Least Absolute Shrinkage and Selectionatoroperator)算法，这里  Absolute 指绝对值。Shrinkage收缩的含义：即系数收缩在一定区域内（比如圆内）。</p>

<p><strong>主要思想</strong>：
通过构造一个一阶惩罚函数获得一个精炼的模型；通过最终确定一些指标（变量）癿系数为零（岭回归估计系数等于0癿机会微乎其微，造成筛选变量困难），解释力很强。擅长处理具有多重共线性癿数据，筛选变量，与岭回归一样是有偏估计。</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/04/215937llmsx53dfu3fdg6l.jpg" alt="" />
<img src="http://attachbak.dataguru.cn/attachments/forum/201603/04/215611cb0x1eudd6obhi8z.jpg" alt="" /></p>

<p><strong>几何解释</strong></p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/04/220811s52a7l3326a351i3.jpg" alt="" /></p>

<p>由于方框的顶点更容易交于抛物面，也就是lasso更易求解，而该顶点对应的很多系数为0，也就是起到了筛选变量的目的。</p>

<h1 id="6-lasso-vs-岭回归">6.LASSO vs 岭回归</h1>

<p>岭回归一方面可以将其变成一个最小二乘问题。另一方面可以将它解释成一个带约束项的系数优化问题。λ增大的过程就是t减小的过程，该图也说明了岭回归系数估计值为什么通常不为0，因为随着抛物面的扩展，它与约束圆的交点可能在圆周上的任意位置，除非交点恰好位于某个坐标轴或坐标平面上，否则大多数情况交点对应的系数值都不为零。再加上λ的选择应使椭球面和圆周的交点恰好在一个坐标平面上，更增加了求解λ的难度。</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/04/222225tr9tt44tn768b45t.jpg" alt="" /></p>

<p>左图为岭回归，右图为lasso回归。横轴越往左，自由度越小（即圆或方框在收缩的过程），λ越大，系数（即纵轴）会越趋于0。但是岭回归没有系数真正为0，但lasso的不断有系数变为0.</p>

<h1 id="7-一般化的模型">7.一般化的模型</h1>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/04/222650guccc8yvc8jnxh7j.jpg" alt="" /></p>

<p>不同q对应的约束域形状</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/04/222758pwesenw44c4ilds5.jpg" alt="" /></p>

<h1 id="8-弹性网模型">8.弹性网模型</h1>

<p>Zouand Hastie (2005)提出elasticnet，介于岭回归和lasso回归之间，现在被认为是处理多重共线性和变量筛选较好的收缩方法，而且损失的精度不会太多。</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/04/223232tlg6lnnnvlxflhmc.jpg" alt="" /></p>

<h1 id="9-最小角回归-least-angel-regression-是lasso-regression癿一种高效解法">9.最小角回归(Least Angel Regression)是lasso regression癿一种高效解法。</h1>

<p>Lasso回归中表达式用偏导求极值时，存在部分点不可导的情况（如方框的尖点），如何解决？</p>

<p>Efron于2004年提出癿一种变量选择癿方法，<strong>类似于</strong>向前逐步回归(Forward Stepwise)的形式，最初用于解决传统的线性回归问题，有清晰的几何意义。</p>

<p>与向前逐步回归(Forward Stepwise)不同点在于，Forward Step wise 每次都是根据选择的变量子集，完全拟合出线性模型，计算出RSS，再设计统计量（如AIC）对较高癿模型复杂度作出惩罚，而LAR是每次先找出和因变量相关度较高的那个变量, 再沿着LSE的方向一点点调整这个predictor的系数，在这个过程中，这个变量和残差的相关系数会逐渐减小，等到这个相关性没那么显著的时候，就要选进新的相关性较高的变量，然后重新沿着LSE的方向进行变动。而到最后，所有变量都被选中，就和LSE相同了。</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/05/103533pkjgm1gjjz0kyj2f.jpg" alt="" /></p>

<p>左图为LAR逐步加上变量的过程（从左往右看），右图为LASSO变量逐渐淘汰的收缩过程（从右往左看）。
对比两幅图，非常类似。所以可以用LAR方法来计算LASSO，该方法完全是线性解决方法，没有迭代的过程。</p>

<h1 id="10-相关系数的几何意义">10. 相关系数的几何意义</h1>

<p>设变量y=[y1,y2,&hellip;yn]; 变量x=[x1,x2,&hellip;,xn].</p>

<p>其相关系数为<img src="http://attachbak.dataguru.cn/attachments/forum/201603/05/104719un7jmmy6q0xx6j0z.jpg" alt="" /></p>

<p>其中cov&ndash;协方差、var&mdash;方差。</p>

<p>如果对x和y进行中心化、标准化，则var(y)=var(x)=1,相关系数变为x1y1+x2y2+&hellip;.+xnyn，即为向量x和y的内积=||x||<em>||y||</em>cos θ，其中θ为x和y的夹角。而对于标准化和中心化后的x和y，则有||x||=||y||=1，所以此时x和y的内积就是它们夹角的余弦。</p>

<ul>
<li><p>如果x和y向量很像，几乎重合，则夹角θ=0，也就是相关系数=内积=1，此时称为高度相关.</p></li>

<li><p>如果x和y相关程度很低，则表现出来的x和y向量相互垂直，相关系数=0.</p></li>

<li><p>如果相关系数=-1，标明x和y呈180°，即负相关。</p></li>
</ul>

<h1 id="11-lar算法及几何意义">11. LAR算法及几何意义</h1>

<p>参考书The Elements of Statistical Learning .pdf的74页。LAR和Lasso的区别以及LAR解Lasso的修正
参考书The Elements of Statistical Learning .pdf的76页。</p>

<p><img src="http://attachbak.dataguru.cn/attachments/forum/201603/05/163655y8vuo2dqdu7jo66b.jpg" alt="" /></p>

<p>假设有6个变量，最先加入与残差向量相关系数较大的浅蓝色v2，在v2变化过程中，相关系数越变越小，直到等于深蓝色的v6，于是加入v6，沿着v2与v6的最小角方向（即向量角分线方向）前进，此后v2和v6与残差向量的相关系数是共同变化的，即两者合并变化，使得相关系数越来越小，直到加入黑色v4为止，三个变量一起变化，&hellip;，一直打到最小二乘解为止，此时残差向量与所有变量的相关系数都为0，即与他们都垂直。</p>

<p>横坐标L1 Length表示：从原点开始走了多长距离，就是值距离，L1范数。</p>

<h1 id="12-r语言中对lar的实现">12. R语言中对LAR的实现</h1>

<pre><code class="language-R">install.packages(&quot;lars&quot;)  #lars包
longley  #用longley数据集，它是一个著名的多重共线性例子
w=as.matrix(longley)  #将数据集转换为一个矩阵

laa=lars(w[,2:7],w[,1]) #w的2:7列为自变量，第1列为因变量
laa  #显示LAR回归过程

##Call:
##lars(x = w[, 2:7], y = w[, 1])
##R-squared: 0.993 
##Sequence of LASSO moves:
##     GNP Year Armed.Forces Unemployed Employed Population Year Employed   Employed Year Employed Employed
##Var    1    5            3                   2                   6           4          -5           -6            6             5  -6        6                                                 
##Step  1    2            3                   4                   5           6           7          8             9             10  11       12 


plot(laa)  #画lasso回归过程图
summary(laa)

#以上结果显示了每一步的残差平方和RSS和多重共线性指标Cp（Mallows's Cp http://en.wikipedia.org/wiki/Mallows%27_Cp）
#Cp越小，多重共线性越小，因此结果以第八步为准，即只剩下第1、2、3、4个变量
</code></pre>


  <footer>
  
<nav class="post-nav">
  <span class="nav-prev">&larr; <a href="/blog/cn/2017/09/coxstratified/">Cox 分层原理</a></span>
  <span class="nav-next"><a href="/blog/cn/2017/09/datamining_concept/">常见的机器学习&amp;数据挖掘知识点</a> &rarr;</span>
</nav>





<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

  



<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/languages/tex.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



  
  <hr>
  <div class="copyright">&copy; <a href="xxxx">Who care</a> 2017 - Forever, maybe</div>
  
  </footer>
  </article>
  
  </body>
</html>


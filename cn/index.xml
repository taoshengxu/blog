<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>中文博客 on My Lives</title>
    <link>/blog/cn/</link>
    <description>Recent content in 中文博客 on My Lives</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Sep 2017 21:10:14 +0000</lastBuildDate>
    <atom:link href="/blog/cn/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>R语言回归函数汇总 </title>
      <link>/blog/cn/2017/09/r_function_regression/</link>
      <pubDate>Fri, 22 Sep 2017 21:10:14 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/r_function_regression/</guid>
      <description>
        

&lt;p&gt;From &lt;a href=&#34;http://blog.sina.com.cn/s/blog_6fbfcfb50102va2k.html&#34;&gt;http://blog.sina.com.cn/s/blog_6fbfcfb50102va2k.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;1-回归的多面性&#34;&gt;1. 回归的多面性&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;回归类型    用途
---
简单线性    个量化的解释变量来预测一个量化的响应变量（一个因变量、一个自变量）
多项式   一个量化的解释变量预测一个量化的响应变量，模型的关系是n阶多项式（一个预测变量，但同时包含变量的幂）
多元线性    用两个或多个量化的解释变量预测一个量化的响应变量（不止一个预测变量）
多变量     用一个或多个解释变量预测多个响应变量
Logistic    用一个或多个解释变量预测一个类别型变量
泊松      用一个或多个解释变量预测一个代表频数的响应变量
Cox         用一个或多个解释变量预测一个事件（死亡、失败或旧病复发）发生的时间
时间序列  对误差项相关的时间序列数据建模
非线性   用一个或多个量化的解释变量预测一个量化的响应变量，不过模型是非线性的
非参数   用一个或多个量化的解释变量预测一个量化的响应变量，模型的形式源自数据形式，不事先设定
稳健      用一个或多个量化的解释变量预测一个量化的响应变量，能抵御强影响点的干扰
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;2-用lm-拟合回归模型&#34;&gt;2. 用lm()拟合回归模型&lt;/h1&gt;

&lt;p&gt;myfit&amp;lt;-lm(formula,data)&lt;/p&gt;

&lt;p&gt;formula指要拟合的模型形式，data是一个数据框，包含了用于拟合模型的数据&lt;/p&gt;

&lt;p&gt;formula形式如下：Y~X1+X2+……+Xk （~左边为响应变量，右边为各个预测变量，预测变量之间用+符号分隔）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R表达式中常用的符号&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;符号 用途
---
~  | 分隔符号，左边为响应变量，右边为解释变量，eg：要通过x、z和w预测y，代码为y~x+z+w
+  | 分隔预测变量
： | 表示预测变量的交互项  eg：要通过x、z及x与z的交互项预测y，代码为y~x+z+x:z
*  | 表示所有可能交互项的简洁方式，代码y~x*z*w可展开为y~x+z+w+x:z+x:w+z:w+x:z:w
^  | 表示交互项达到某个次数，代码y~(x+z+w)^2可展开为y~x+z+w+x:z+x:w+z:w
.  | 表示包含除因变量外的所有变量，eg：若一个数据框包含变量x、y、z和w，代码y~.可展开为y~x+z+w
-  | 减号，表示从等式中移除某个变量，eg：y~(x+z+w)^2-x:w可展开为y~x+z+w+x:z+z:w
-1 | 删除截距项，eg：表示y~x-1拟合y在x上的回归，并强制直线通过原点
I（） | 从算术的角度来解释括号中的元素。Eg：y~x+(z+w)^2将展开为y~x+z+w+z:w。相反，代码y~x+I((z+w)^2)将展开为y~x+h，h是一个由z和w的平方和创建的新变量
function | 可以在表达式中用的数学函数，例如log(y)~x+z+w表示通过x、z和w来预测log(y)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;交互项&lt;/strong&gt;是指你的几个变量一块生成了一个新的影响，比如不同性别的不同专业可能会对成绩有不同的影响，性别影响成绩，专业影响成绩，但是性别和专业和在一起又产生新影响。这时候就需要交互项。具体用不用看你的方程,一般不用。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;对拟合线性模型非常有用的其他函数&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;函数 用途
---
Summary（）展示拟合的详细结果
Coefficients（）列出拟合模型的模型参数（截距项和斜率）
Cofint（） 提供模型参数的置信区间（默认95%）
Fitted（）列出拟合模型的预测值
Residuals（）列出拟合模型的残差值
Anova（）生成一个拟合模型的方差分析，或者比较两个或更多拟合模型的方差分析表
Vcov（）列出模型参数的协方差矩阵
AIC（）输出赤池信息统计量
Plot（）生成评价拟合模型的诊断图
Predict（）用拟合模型对新的数据集预测响应变量值
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;3-r-语言示例lm&#34;&gt;3. R 语言示例lm()&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fit&amp;lt;-lm(weight~height,data=women)  
summary(fit)
# 在Pr(&amp;gt;|t|)栏，可以看到回归系数（3.45）显著不为0（p&amp;lt;0.001），表明身高每增加1英寸，体重将预期地增加3.45磅
# R平方项（0.991）表明模型可以解释体重99.1%的方差，它也是实际和预测值之间的相关系数（R^2=r^2）
# 残差的标准误（1.53lbs）则可认为模型用身高预测体重的平均误差
# F统计量检验所有的预测变量预测响应变量是否都在某个几率水平之上

fitted(fit)#拟合模型的预测值  
residuals(fit)#拟合模型的残差值 
plot(women$height,women$weight,  
     xlab=&amp;quot;Height （in inches）&amp;quot;,  
     ylab=&amp;quot;Weight（in pounds）&amp;quot;)  
abline(fit)  
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;4-多项式回归&#34;&gt;4.多项式回归&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fit2&amp;lt;-lm(weight~height+I(height^2),data=women)  
summary(fit2)
plot(women$height,women$weight,  
     xlab=&amp;quot;Height（in inches）&amp;quot;,  
     ylab=&amp;quot;Weight（in lbs）&amp;quot;)  
lines(women$height,fitted(fit2)) 

#一般来说，n次多项式生成一个n-1个弯曲的曲线
#car包中的scatterplot（）函数，可以很容易、方便地绘制二元关系图
library(car)
scatterplot(weight~height,  
            data=women,  
            spread=FALSE,  
            lty.smooth=2,  
            pch=19,  
            main=&amp;quot;Women Age 30-39&amp;quot;,  
            xlab=&amp;quot;Height (inches)&amp;quot;,  
            ylab=&amp;quot;Weight(lbs.)&amp;quot;) 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;6-多元线性回归&#34;&gt;6.多元线性回归&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;states&amp;lt;-as.data.frame(state.x77[,c(&amp;quot;Murder&amp;quot;,&amp;quot;Population&amp;quot;,&amp;quot;Illiteracy&amp;quot;,&amp;quot;Income&amp;quot;,&amp;quot;Frost&amp;quot;)])
cor(states)  
scatterplotMatrix(states,spread=FALSE,lty.smooth=2,main=&amp;quot;Scatter Plot Matrix&amp;quot;)
#scatterplotMatrix（）函数默认在非对角线区域绘制变量间的散点图，并添加平滑（loess）和线性拟合曲线
#多元线性回归
fit&amp;lt;-lm(Murder~Population+Illiteracy+Income+Frost,data=states)  
summary(fit)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;7-有交互项的多元线性回归&#34;&gt;7.有交互项的多元线性回归&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fit&amp;lt;-lm(mpg~hp+wt+hp:wt,data=mtcars)  
summary(fit)  
# 通过effects包中的effect（）函数，可以用图形展示交互项的结果
install.packages(&amp;quot;effects&amp;quot;)  
library(effects)  
plot(effect(&amp;quot;hp:wt&amp;quot;,fit,  
            list(wt=c(2.2,3.2,4.2))),multiline=TRUE)
            
# 二次拟合诊断图
fit2&amp;lt;-lm(weight~height+I(height^2),data=women)  
par(mfrow=c(2,2))  
plot(fit2)           
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;8-q-q-图-quantile-quantile&#34;&gt;8. Q-Q 图(Quantile - Quantile)&lt;/h1&gt;

&lt;p&gt;在验证一份数据是否属于正态分布的时候，我们经常会先画一幅Q-Q图，视觉上感受一下这份数据是否属于正态分布的，只要大部分的数据都飘在一条直线上，那么我们就可以基本上肯定这份数据是正态分布的了。&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>R语言 逐步回归分析</title>
      <link>/blog/cn/2017/09/step_regression/</link>
      <pubDate>Fri, 22 Sep 2017 20:10:14 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/step_regression/</guid>
      <description>
        

&lt;p&gt;From &lt;a href=&#34;http://www.cnblogs.com/liuzezhuang/p/3724497.html&#34;&gt;http://www.cnblogs.com/liuzezhuang/p/3724497.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;逐步回归分析是以AIC信息统计量为准则，通过选择最小的AIC信息统计量，来达到删除或增加变量的目的。&lt;/p&gt;

&lt;p&gt;R语言中用于逐步回归分析的函数 step()    drop1()     add1()&lt;/p&gt;

&lt;h1 id=&#34;1-载入数据&#34;&gt;1.载入数据&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#首先对数据进行多元线性回归分析
tdata&amp;lt;-data.frame(
  x1=c( 7, 1,11,11, 7,11, 3, 1, 2,21, 1,11,10),
  x2=c(26,29,56,31,52,55,71,31,54,47,40,66,68),
  x3=c( 6,15, 8, 8, 6, 9,17,22,18, 4,23, 9, 8),
  x4=c(60,52,20,47,33,22, 6,44,22,26,34,12,12),
  Y =c(78.5,74.3,104.3,87.6,95.9,109.2,102.7,72.5,
       93.1,115.9,83.8,113.3,109.4)
)
tlm&amp;lt;-lm(Y~x1+x2+x3+x4,data=tdata)
summary(tlm)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://images.cnitblog.com/i/388224/201405/122302215787851.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过观察，回归方程的系数都没有通过显著性检验&lt;/p&gt;

&lt;p&gt;#2.逐步回归分析&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tstep&amp;lt;-step(tlm)
summary(tstep)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://images.cnitblog.com/i/388224/201405/122307143595998.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;结果分析：
* 当用x1 x2 x3 x4作为回归方程的系数时，AIC的值为26.94&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;去掉x3 回归方程的AIC值为24.974；去掉x4 回归方程的AIC值为25.011……&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;由于去x3可以使得AIC达到最小值，因此R会自动去掉x3;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;*去掉x3之后 AIC的值都增加 逐步回归分析终止,  得到当前最优的回归方程&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://images.cnitblog.com/i/388224/201405/122315069535378.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;回归系数的显著性水平有所提高 但是x2 x4的显著性水平仍然不理想&lt;/p&gt;

&lt;p&gt;#3.逐步回归分析的优化&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;drop1(tstep)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://images.cnitblog.com/i/388224/201405/122319369845108.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;#4.进一步进行多元回归分析&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tlm&amp;lt;-lm(Y~x1+x2,data=tdata)
summary(tlm)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://images.cnitblog.com/i/388224/201405/122323393753359.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;所有的检验均为显著&lt;/p&gt;

&lt;p&gt;因此所得回归方程为&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;y=52.57735+ 1.46831x1+ 0.66225x2.
&lt;/code&gt;&lt;/pre&gt;

        
      </description>
    </item>
    
    <item>
      <title>模型选择准则之AIC和BIC</title>
      <link>/blog/cn/2017/09/aic_bic/</link>
      <pubDate>Fri, 22 Sep 2017 19:47:14 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/aic_bic/</guid>
      <description>
        

&lt;p&gt;转自：&lt;a href=&#34;http://blog.csdn.net/jteng/article/details/40823675&#34;&gt;http://blog.csdn.net/jteng/article/details/40823675&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;很多参数估计问题均采用似然函数作为目标函数，当训练数据足够多时，可以不断提高模型精度，但是以提高模型复杂度为代价的，同时带来一个机器学习中非常普遍的问题——过拟合。所以，模型选择问题在模型复杂度与模型对数据集描述能力（即似然函数）之间寻求最佳平衡。
人们提出许多信息准则，通过加入模型复杂度的惩罚项来避免过拟合问题，此处我们介绍一下常用的两个模型选择方法.&lt;/p&gt;

&lt;h1 id=&#34;赤池信息准则-akaike-information-criterion-aic&#34;&gt;赤池信息准则（Akaike Information Criterion，AIC）&lt;/h1&gt;

&lt;p&gt;AIC是衡量统计模型拟合优良性的一种标准，由日本统计学家赤池弘次在1974年提出，它建立在熵的概念上，提供了权衡估计模型复杂度和拟合数据优良性的标准。
通常情况下，AIC定义为：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;AIC=2k-2ln(L)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中k是模型参数个数，L是似然函数。从一组可供选择的模型中选择最佳模型时，通常选择AIC最小的模型。
当两个模型之间存在较大差异时，差异主要体现在似然函数项，当似然函数差异不显著时，上式第一项，即模型复杂度则起作用，从而参数个数少的模型是较好的选择。
一般而言，当模型复杂度提高（k增大）时，似然函数L也会增大，从而使AIC变小，但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。目标是选取AIC最小的模型，AIC不仅要提高模型拟合度（极大似然），而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。&lt;/p&gt;

&lt;h1 id=&#34;贝叶斯信息准则-bayesian-information-criterion-bic&#34;&gt;贝叶斯信息准则（Bayesian Information Criterion，BIC）&lt;/h1&gt;

&lt;p&gt;BIC（Bayesian InformationCriterion）贝叶斯信息准则与AIC相似，用于模型选择，1978年由Schwarz提出。训练模型时，增加参数数量，也就是增加模型复杂度，会增大似然函数，但是也会导致过拟合现象，针对该问题，AIC和BIC均引入了与模型参数个数相关的惩罚项，BIC的惩罚项比AIC的大，考虑了样本数量，样本数量过多时，可有效防止模型精度过高造成的模型复杂度过高。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-math&#34;&gt;BIC=kln(n)-2ln(L)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中，k为模型参数个数，n为样本数量，L为似然函数。kln(n)惩罚项在维数过大且训练样本数据相对较少的情况下，可以有效避免出现维度灾难现象。&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>SSE, MSE, RMSE, R-square</title>
      <link>/blog/cn/2017/09/sse_mse_rmse_r-square/</link>
      <pubDate>Fri, 22 Sep 2017 16:48:14 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/sse_mse_rmse_r-square/</guid>
      <description>
        

&lt;pre&gt;&lt;code&gt;SSE(和方差、误差平方和)：The sum of squares due to error
MSE(均方误、方差)：Mean squared error
RMSE(均方根、标准差)：Root mean squared error
R-square(确定系数)：Coefficient of determination
Adjusted R-square：Degree-of-freedom adjusted coefficient of determination
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;sse-和方差-误差平方和&#34;&gt;SSE(和方差、误差平方和)&lt;/h1&gt;

&lt;p&gt;拟合数据和原始数据对应点的误差的平方和&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/628033fa48bb1eb1a0879&amp;amp;690.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SSE越接近于0，说明模型选择和拟合更好，数据预测也越成功。接下来的MSE和RMSE因为和SSE是同出一宗，所以效果一样。&lt;/p&gt;

&lt;h1 id=&#34;mse-均方误&#34;&gt;MSE(均方误)&lt;/h1&gt;

&lt;p&gt;预测数据和原始数据对应点误差的平方和的均值，也就是SSE/n，和SSE没有太大的区别，&lt;strong&gt;最常用&lt;/strong&gt;！&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/628033fa48bb1ebfee394&amp;amp;690.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;rmse-均方根&#34;&gt;RMSE(均方根)&lt;/h1&gt;

&lt;p&gt;回归系统的拟合标准差，是MSE的平方根，就算公式如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/628033fa48bb1ec86542a690.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;在这之前，我们所有的误差参数都是基于预测值(y&lt;em&gt;hat)和原始值(y)之间的误差(即点对点)。从下面开始是所有的误差都是相对原始数据平均值(y&lt;/em&gt;ba)而展开的(即点对全)&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;r-square-确定系数&#34;&gt;R-square(确定系数)&lt;/h1&gt;

&lt;h4 id=&#34;1-ssr-sum-of-squares-of-the-regression-即预测数据与原始数据均值之差的平方和&#34;&gt;(1)SSR：Sum of squares of the regression，即预测数据与原始数据均值之差的平方和&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/628033fa48bb1ed5dd403&amp;amp;690.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-sst-total-sum-of-squares-即原始数据和均值之差的平方和&#34;&gt;(2)SST：Total sum of squares，即原始数据和均值之差的平方和&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/628033fa48bb1ede9b464&amp;amp;690.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SST=SSE+SSR&lt;/p&gt;

&lt;p&gt;“确定系数”是定义为SSR和SST的比值&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/628033fa48bb1ee8ee0fa&amp;amp;690.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;R-square是通过数据的变化来表征一个拟合的好坏。由上面的表达式可以知道“确定系数”的正常取值范围为[0 1]，越接近1，表明方程的变量对y的解释能力越强，这个模型对数据拟合的也较好。&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/MSE_RMSE.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/MAE.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>常见的机器学习&amp;数据挖掘知识点</title>
      <link>/blog/cn/2017/09/datamining_concept/</link>
      <pubDate>Fri, 22 Sep 2017 16:47:14 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/datamining_concept/</guid>
      <description>
        

&lt;h1 id=&#34;basis-基础&#34;&gt;Basis(基础)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;SSE(Sum of Squared Error, 平方误差和)
SAE(Sum of Absolute Error, 绝对误差和)
SRE(Sum of Relative Error, 相对误差和)
MSE(Mean Squared Error, 均方误差)
RMSE(Root Mean Squared Error, 均方根误差)
RRSE(Root Relative Squared Error, 相对平方根误差)
MAE(Mean Absolute Error, 平均绝对误差)
RAE(Root Absolute Error, 平均绝对误差平方根)
MRSE(Mean Relative Square Error, 相对平均误差)
RRSE(Root Relative Squared Error, 相对平方根误差)
Expectation(期望)&amp;amp;Variance(方差)
Standard Deviation(标准差，也称Root Mean Squared Error, 均方根误差)
CP(Conditional Probability, 条件概率)
JP(Joint Probability, 联合概率)
MP(Marginal Probability, 边缘概率)
Bayesian Formula(贝叶斯公式)
CC(Correlation Coefficient, 相关系数)
Quantile (分位数)
Covariance(协方差矩阵)
GD(Gradient Descent, 梯度下降)
SGD(Stochastic Gradient Descent, 随机梯度下降)
LMS(Least Mean Squared, 最小均方)
LSM(Least Square Methods, 最小二乘法)
NE(Normal Equation, 正规方程)
MLE(Maximum Likelihood Estimation, 极大似然估计)
QP(Quadratic Programming, 二次规划)
L1 /L2 Regularization(L1/L2正则, 以及更多的, 现在比较火的L2.5正则等)
Eigenvalue(特征值)
Eigenvector(特征向量)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;common-distribution-常见分布&#34;&gt;Common Distribution(常见分布)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;Discrete Distribution(离散型分布)：
Bernoulli Distribution/Binomial Distribution(贝努利分布/二项分布)
Negative Binomial Distribution(负二项分布)
Multinomial Distribution(多项分布)
Geometric Distribution(几何分布)
Hypergeometric Distribution(超几何分布)
Poisson Distribution (泊松分布)
Continuous Distribution (连续型分布)：

Uniform Distribution(均匀分布)
Normal Distribution/Gaussian Distribution(正态分布/高斯分布)
Exponential Distribution(指数分布)
Lognormal Distribution(对数正态分布)
Gamma Distribution(Gamma分布)
Beta Distribution(Beta分布)
Dirichlet Distribution(狄利克雷分布)
Rayleigh Distribution(瑞利分布)
Cauchy Distribution(柯西分布)
Weibull Distribution (韦伯分布)
Three Sampling Distribution(三大抽样分布)：

Chi-square Distribution(卡方分布)
t-distribution(t-分布)
F-distribution(F-分布)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;data-pre-processing-数据预处理&#34;&gt;Data Pre-processing(数据预处理)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;Missing Value Imputation(缺失值填充)
Discretization(离散化)
Mapping(映射)
Normalization(归一化/标准化)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;sampling-采样&#34;&gt;Sampling(采样)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;Simple Random Sampling(简单随机采样)
Offline Sampling(离线等可能K采样)
Online Sampling(在线等可能K采样)
Ratio-based Sampling(等比例随机采样)
Acceptance-rejection Sampling(接受-拒绝采样)
Importance Sampling(重要性采样)
MCMC(Markov Chain MonteCarlo 马尔科夫蒙特卡罗采样算法：Metropolis-Hasting&amp;amp; Gibbs)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;clustering-聚类&#34;&gt;Clustering(聚类)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;K-MeansK-Mediods
二分K-Means
FK-Means
Canopy
Spectral-KMeans(谱聚类)
GMM-EM(混合高斯模型-期望最大化算法解决)
K-Pototypes
CLARANS(基于划分)
BIRCH(基于层次)
CURE(基于层次)
STING(基于网格)
CLIQUE(基于密度和基于网格)
2014年Science上的密度聚类算法等
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;clustering-effectiveness-evaluation-聚类效果评估&#34;&gt;Clustering Effectiveness Evaluation(聚类效果评估)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;Purity(纯度)
RI(Rand Index, 芮氏指标)
ARI(Adjusted Rand Index, 调整的芮氏指标)
NMI(Normalized Mutual Information, 规范化互信息)
F-meaure(F测量)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;classification-regression-分类-回归&#34;&gt;Classification&amp;amp;Regression(分类&amp;amp;回归)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;LR(Linear Regression, 线性回归)
LR(Logistic Regression, 逻辑回归)
SR(Softmax Regression, 多分类逻辑回归)
GLM(Generalized Linear Model, 广义线性模型)
RR(Ridge Regression, 岭回归/L2正则最小二乘回归)，LASSO(Least Absolute Shrinkage and Selectionator Operator , L1正则最小二乘回归)
DT(Decision Tree决策树)
RF(Random Forest, 随机森林)
GBDT(Gradient Boosting Decision Tree, 梯度下降决策树)
CART(Classification And Regression Tree 分类回归树)
KNN(K-Nearest Neighbor, K近邻)
SVM(Support Vector Machine, 支持向量机, 包括SVC(分类)&amp;amp;SVR(回归))
CBA(Classification based on Association Rule, 基于关联规则的分类)
KF(Kernel Function, 核函数) 
Polynomial Kernel Function(多项式核函数)
Guassian Kernel Function(高斯核函数)
Radial Basis Function(RBF径向基函数)
String Kernel Function 字符串核函数
NB(Naive Bayesian,朴素贝叶斯)
BN(Bayesian Network/Bayesian Belief Network/Belief Network 贝叶斯网络/贝叶斯信度网络/信念网络)
LDA(Linear Discriminant Analysis/Fisher Linear Discriminant 线性判别分析/Fisher线性判别)
EL(Ensemble Learning, 集成学习) 
Boosting
Bagging
Stacking
AdaBoost(Adaptive Boosting 自适应增强)
MEM(Maximum Entropy Model, 最大熵模型)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;classification-effectivenessevaluation-分类效果评估&#34;&gt;Classification EffectivenessEvaluation(分类效果评估)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;Confusion Matrix(混淆矩阵)
Precision(精确度)
Recall(召回率)
Accuracy(准确率)
F-score(F得分)
ROC Curve(ROC曲线)
AUC(AUC面积)
Lift Curve(Lift曲线)
KS Curve(KS曲线)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;pgm-probabilistic-graphical-models-概率图模型&#34;&gt;PGM(Probabilistic Graphical Models, 概率图模型)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;BN(BayesianNetwork/Bayesian Belief Network/ Belief Network , 贝叶斯网络/贝叶斯信度网络/信念网络)
MC(Markov Chain, 马尔科夫链)
MEM(Maximum Entropy Model, 最大熵模型)
HMM(Hidden Markov Model, 马尔科夫模型)
MEMM(Maximum Entropy Markov Model, 最大熵马尔科夫模型)
CRF(Conditional Random Field,条件随机场)
MRF(Markov Random Field, 马尔科夫随机场)
Viterbi(维特比算法)
NN(Neural Network, 神经网络)

ANN(Artificial Neural Network, 人工神经网络)
SNN(Static Neural Network, 静态神经网络)
BP(Error Back Propagation, 误差反向传播)
HN(Hopfield Network)
DNN(Dynamic Neural Network, 动态神经网络)
RNN(Recurrent Neural Network, 循环神经网络)
SRN(Simple Recurrent Network, 简单的循环神经网络)
ESN(Echo State Network, 回声状态网络)
LSTM(Long Short Term Memory, 长短记忆神经网络)
CW-RNN(Clockwork-Recurrent Neural Network, 时钟驱动循环神经网络, 2014ICML）等.
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;deep-learning-深度学习&#34;&gt;Deep Learning(深度学习)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;Auto-encoder(自动编码器)
SAE(Stacked Auto-encoders堆叠自动编码器) 
Sparse Auto-encoders(稀疏自动编码器)
Denoising Auto-encoders(去噪自动编码器)
Contractive Auto-encoders(收缩自动编码器)
RBM(Restricted Boltzmann Machine, 受限玻尔兹曼机)
DBN(Deep Belief Network, 深度信念网络)
CNN(Convolutional Neural Network, 卷积神经网络)
Word2Vec(词向量学习模型)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;dimensionality-reduction-降维&#34;&gt;Dimensionality Reduction(降维)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;LDA(Linear Discriminant Analysis/Fisher Linear Discriminant, 线性判别分析/Fish线性判别)
PCA(Principal Component Analysis, 主成分分析)
ICA(Independent Component Analysis, 独立成分分析)
SVD(Singular Value Decomposition 奇异值分解)
FA(Factor Analysis 因子分析法)
Text Mining(文本挖掘)：

VSM(Vector Space Model, 向量空间模型)
Word2Vec(词向量学习模型)
TF(Term Frequency, 词频)
TF-IDF(TermFrequency-Inverse Document Frequency, 词频-逆向文档频率)
MI(Mutual Information, 互信息)
ECE(Expected Cross Entropy, 期望交叉熵)
QEMI(二次信息熵)
IG(Information Gain, 信息增益)
IGR(Information Gain Ratio, 信息增益率)
Gini(基尼系数)
x2 Statistic(x2统计量)
TEW(Text Evidence Weight, 文本证据权)
OR(Odds Ratio, 优势率)
N-Gram Model
LSA(Latent Semantic Analysis, 潜在语义分析)
PLSA(Probabilistic Latent Semantic Analysis, 基于概率的潜在语义分析)
LDA(Latent Dirichlet Allocation, 潜在狄利克雷模型)
SLM(Statistical Language Model, 统计语言模型)
NPLM(Neural Probabilistic Language Model, 神经概率语言模型)
CBOW(Continuous Bag of Words Model, 连续词袋模型)
Skip-gram(Skip-gram Model)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;association-mining-关联挖掘&#34;&gt;Association Mining(关联挖掘)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;Apriori算法
FP-growth(Frequency Pattern Tree Growth, 频繁模式树生长算法)
MSApriori(Multi Support-based Apriori, 基于多支持度的Apriori算法)
GSpan(Graph-based Substructure Pattern Mining, 频繁子图挖掘)
Sequential Patterns Analysis(序列模式分析)

AprioriAll
Spade
GSP(Generalized Sequential Patterns, 广义序列模式)
PrefixSpan
Forecast(预测)

LR(Linear Regression, 线性回归)
SVR(Support Vector Regression, 支持向量机回归)
ARIMA(Autoregressive Integrated Moving Average Model, 自回归积分滑动平均模型)
GM(Gray Model, 灰色模型)
BPNN(BP Neural Network, 反向传播神经网络)
SRN(Simple Recurrent Network, 简单循环神经网络)
LSTM(Long Short Term Memory, 长短记忆神经网络)
CW-RNN(Clockwork Recurrent Neural Network, 时钟驱动循环神经网络)
……
Linked Analysis(链接分析)

HITS(Hyperlink-Induced Topic Search, 基于超链接的主题检索算法)
PageRank(网页排名)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;recommendation-engine-推荐引擎&#34;&gt;Recommendation Engine(推荐引擎)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;SVD
Slope One
DBR(Demographic-based Recommendation, 基于人口统计学的推荐)
CBR(Context-based Recommendation, 基于内容的推荐)
CF(Collaborative Filtering, 协同过滤)
UCF(User-based Collaborative Filtering Recommendation, 基于用户的协同过滤推荐)
ICF(Item-based Collaborative Filtering Recommendation, 基于项目的协同过滤推荐)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;similarity-measure-distance-measure-相似性与距离度量&#34;&gt;Similarity Measure&amp;amp;Distance Measure(相似性与距离度量)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;EuclideanDistance(欧式距离)
Chebyshev Distance(切比雪夫距离)
Minkowski Distance(闵可夫斯基距离)
Standardized EuclideanDistance(标准化欧氏距离)
Mahalanobis Distance(马氏距离)
Cos(Cosine, 余弦)
Hamming Distance/Edit Distance(汉明距离/编辑距离)
Jaccard Distance(杰卡德距离)
Correlation Coefficient Distance(相关系数距离)
Information Entropy(信息熵)
KL(Kullback-Leibler Divergence, KL散度/Relative Entropy, 相对熵)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;optimization-最优化&#34;&gt;Optimization(最优化)：&lt;/h1&gt;

&lt;h3 id=&#34;non-constrained-optimization-无约束优化&#34;&gt;Non-constrained Optimization(无约束优化)：&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;Cyclic Variable Methods(变量轮换法)
Variable Simplex Methods(可变单纯形法)
Newton Methods(牛顿法)
Quasi-Newton Methods(拟牛顿法)
Conjugate Gradient Methods(共轭梯度法)。
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;constrained-optimization-有约束优化&#34;&gt;Constrained Optimization(有约束优化)：&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;Approximation Programming Methods(近似规划法)
Penalty Function Methods(罚函数法)
Multiplier Methods(乘子法)。
Heuristic Algorithm(启发式算法)
SA(Simulated Annealing, 模拟退火算法)
GA(Genetic Algorithm, 遗传算法)
ACO(Ant Colony Optimization, 蚁群算法)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;feature-selection-特征选择&#34;&gt;Feature Selection(特征选择)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;Mutual Information(互信息)
Document Frequence(文档频率)
Information Gain(信息增益)
Chi-squared Test(卡方检验)
Gini(基尼系数)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;outlier-detection-异常点检测&#34;&gt;Outlier Detection(异常点检测)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;Statistic-based(基于统计)
Density-based(基于密度)
Clustering-based(基于聚类)。
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;learning-to-rank-基于学习的排序&#34;&gt;Learning to Rank(基于学习的排序)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;Pointwise 
McRank
Pairwise 
RankingSVM
RankNet
Frank
RankBoost；
Listwise 
AdaRank
SoftRank
LamdaMART
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;tool-工具&#34;&gt;Tool(工具)：&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;MPI
Hadoop生态圈
Spark
IGraph
BSP
Weka
Mahout
Scikit-learn
PyBrain
Theano 
&lt;/code&gt;&lt;/pre&gt;

        
      </description>
    </item>
    
    <item>
      <title>线性回归-岭回归-Lasso-弹性网-多重共线性</title>
      <link>/blog/cn/2017/09/ridgelasso/</link>
      <pubDate>Fri, 22 Sep 2017 09:47:14 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/ridgelasso/</guid>
      <description>
        

&lt;p&gt;&lt;a href=&#34;http://f.dataguru.cn/thread-598486-1-1.html&#34;&gt;原文&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;1-回归问题的数学描述&#34;&gt;1. 回归问题的数学描述&lt;/h1&gt;

&lt;p&gt;1.n个样本，p个变量，X，y已知。对数据中心化、标准化处理后，可以去掉截距项。
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/145150y9utqyr9p0s99pk0.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2.矩阵形式的多元线性模型为:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/145822oknvx60lzl92v691.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;求解β，使得误差项ε能达到较低.&lt;/p&gt;

&lt;p&gt;3.残差平方和RSS为&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/151838z32fr356n52in5m3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4.多元线性回归问题变为求解β，从而使残差平方和极小值问题（关于系数向量β的二次函数极值问题）&lt;/p&gt;

&lt;p&gt;5.几何意义&lt;/p&gt;

&lt;p&gt;残差向量的几何意义：响应y向量到由p个x向量组成的超平面的距离向量。&lt;br&gt;
残差平方和几何意义：残差向量长度的平方。&lt;/p&gt;

&lt;h1 id=&#34;2-最小二乘回归&#34;&gt;2.最小二乘回归&lt;/h1&gt;

&lt;p&gt;使用最小二乘法拟合的普通线性回归是数据建模的基本方法。其建模要点在于误差项一般要求独立同分布（常假定为正态）零均值。t检验用来检验拟合的模型系数的显著性，F检验用来检验模型的显著性（方差分析）。如果正态性不成立，t检验和F检验就没有意义。&lt;/p&gt;

&lt;p&gt;β的最小二乘估计为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/170846uzhmw4wrcvwhkazy.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在统计学上，可证明β的最小二乘解为无偏估计，即多次得到的采样值X而计算出来的多个系数估计值向量的平均值将无限接近于真实值向量β。&lt;/p&gt;

&lt;p&gt;如果存在较强的共线性，即X中各列向量之间存在较强的相关性，会导致|X^T X|≈0, 从而引起对角线上的值很大(X^T X的逆矩阵不不存在)&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;问题-x矩阵不存在广义逆-即奇异性-的情况&#34;&gt;问题： X矩阵不存在广义逆（即奇异性）的情况。&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;X本身存在线性相关关系（即多重共线性），即非满秩矩阵。当采样值误差造成本身线性相关的样本矩阵仍然可以求出逆阵时，此时的逆阵非常不稳定，所求的解也没有什么意义。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;当变量比样本多，即p&amp;gt;n时.回归系数会变得很大，无法求解。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;对较复杂的数据建模-比如文本分类-图像去噪或者基因组研究-的时候-普通线性回归会有一些问题&#34;&gt;对较复杂的数据建模（比如文本分类，图像去噪或者基因组研究）的时候，普通线性回归会有一些问题：&lt;/h5&gt;

&lt;p&gt;（1）预测精度的问题 如果响应变量和预测变量之间有比较明显的线性关系，最小二乘回归会有很小的偏倚，特别是如果观测数量n远大于预测变量p时，最小二乘回归也会有较小的方差。但是如果n和p比较接近，则容易产生过拟合；如果n&amp;lt;p，最小二乘回归得不到有意义的结果。&lt;/p&gt;

&lt;p&gt;（2）模型解释能力的问题 包括在一个多元线性回归模型里的很多变量可能是和响应变量无关的；也有可能产生多重共线性的现象：即多个预测变量之间明显相关。这些情况都会增加模型的复杂程度，削弱模型的解释能力。这时候需要进行变量选择（特征选择）。&lt;/p&gt;

&lt;h4 id=&#34;针对ols-ordinary-least-squares-的问题-在变量选择方面有三种扩展的方法&#34;&gt;针对OLS (ordinary least squares)的问题，在变量选择方面有三种扩展的方法：&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;（1）子集选择 这是传统的方法，包括逐步回归和最优子集法等，对可能的部分子集拟合线性模型，利用判别准则 （如AIC,BIC,Cp,调整R2 等）决定最优的模型。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;（2）收缩方法（shrinkage method） 收缩方法又称为&lt;strong&gt;正则化（regularization）&lt;/strong&gt;。主要是岭回归（ridge regression）和lasso回归。通过对最小二乘估计加入罚约束，使某些系数的估计为0。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;(3)维数缩减 主成分回归（PCR）和偏最小二乘回归（PLS）的方法。把p个预测变量投影到m维空间（m&amp;lt;p），利用投影得到的不相关的组合建立线性模型。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;3-岭回归-ridge-regression-rr-1962&#34;&gt;3.岭回归（Ridge Regression，RR, 1962）&lt;/h1&gt;

&lt;p&gt;思路：在原先的β的最小二乘估计中加一个小扰动λI，是原先无法求广义逆的情况变成可以求出其广义逆，使得问题稳定并得以求解。&lt;/p&gt;

&lt;p&gt;极值问题：
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/083631kd5q6mjtdjwyuy15.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/083802zffpx1zf6mlzf3ax.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;对上式用偏导数求极值，结果就是&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/160953w8k63tvoa76wpw18.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其中&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/083451yxgvv2oacxvaaau4.jpg&#34; alt=&#34;&#34; /&gt;为惩罚函数，它保证了β值不会变的很大。岭参数λ不同，岭回归系数也会不同。&lt;/p&gt;

&lt;p&gt;岭回归是回归参数β的有偏估计。它的结果是使得残差平和变大，但是会使系数检验变好，即R语言summary结果中变量后的*变多。&lt;/p&gt;

&lt;p&gt;岭回归缺陷:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1.主要靠目测选择岭参数&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2.计算岭参数时，各种方法结果差异较大&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以一般认为，岭迹图只能看多重共线性，却很难做变量筛选&lt;/p&gt;

&lt;h1 id=&#34;4-几何解释&#34;&gt;4.几何解释&lt;/h1&gt;

&lt;p&gt;以两个变量为例，系数β1和β2已经经过标准化。残差平方和RSS可以表示为β1和β2的一个二次函数，数学上可以用一个抛物面表示。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;最小二乘法&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/153022dvegnfni1s9d9eov.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;2.岭回归&lt;/p&gt;

&lt;p&gt;约束项为 β1^2+β2^2≤t&lt;/p&gt;

&lt;p&gt;对应着投影为β1和β2平面上的一个圆，即下图中的圆柱.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/154311l7qrvvmtq93grvmw.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;该圆柱与抛物面的交点对应的β1、β2值，即为满足约束项条件下的能取得的最小的β1和β2.&lt;/p&gt;

&lt;p&gt;从β1,β2平面理解，即为抛物面等高线在水平面的投影和圆的交点，如下图所示,可见岭回归解与原先的最小二乘解是有一定距离的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://attachbak.dataguru.cn/attachments/forum/201603/03/110921ybbab0oq9tbn9qpf.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;3.岭回归性质&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/Ridge20170922104329.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;4.岭迹图&lt;/p&gt;

&lt;p&gt;岭迹图作用：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1）观察λ较佳取值；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2）观察变量是否有多重共线性；&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;是λ的函数，岭迹图的横坐标为λ，纵坐标为β(λ)。而β(λ)是一个向量，由β1(λ)、β2(λ)、&amp;hellip;等很多分量组成，每一个分量都是λ的函数，将每一个分量分别用一条线。当不存在奇异性时，岭迹应是稳定地逐渐趋向于0。
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/173356ws06xw7xqov8wc0o.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;岭迹图比较&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/Ridge20170922154100.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;通过岭迹的形状来判断我们是否要剔除掉该参数（例如：岭迹波动很大，说明该变量参数有共线性）&lt;/p&gt;

&lt;p&gt;可见，在λ很小时，通常各β系数取值较大；而如果λ=0，则跟普通意义的多元线性回归的最小二乘解完全一样；当λ略有增大，则各β系数取值迅速减小，即从不稳定趋于稳定。上图类似喇叭形状的岭迹图，一般都存在多重共线性。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;λ的选择：一般通过观察，选取喇叭口附近的值，此时各β值已趋于稳定，但总的RSS又不是很大。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;选择变量：删除那些β取值一直趋于0的变量。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;注意：用岭迹图筛选变量并非十分靠谱。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;岭回归选择变量的原则（不靠谱，仅供参考）
* 1）在岭回归中设计矩阵X已经中心化和标准化了，这样可以直接比较标准化岭回归系数癿大小。可以剔除掉标准化岭回归系数比较稳定且值很小癿自变量。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;2）随着λ的增加，回归系数不稳定，震动趋于零的自变量也可以剔除。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;3）如果依照上述去掉变量的原则，有若干个回归系数不稳定，究竟去掉几个，去掉哪几个，这幵无一般原则可循，这需根据去掉某个变量后重新进行岭回归分析的效果来确定。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;5.岭回归R语言分析&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(MASS)#岭回归在MASS包中。
longley #内置数据集，有关国民经济情况的数据，以多重共线性较强著称
summary(fm1&amp;lt;-lm(Employed~.,data=longley)) #最小二乘估计的多元线性回归
#结果可见，R^2很高，但是系数检验不是非常理想
names(longley)[1]&amp;lt;-&amp;quot;y&amp;quot;  
lm.ridge(y~.,longley)   #此时，仍为线性回归
plot(lm.ridge(y~.,longley,lambda=seq(0,0.1,0.001)))  #加了参数lambda的描述后才画出响应的岭迹图
#由于lambda趋于0时，出现了不稳定的情况，所以可以断定变量中存在多重共线性
select(lm.ridge(y~.,longley,lambda=seq(0,0.1,0.001)))  #用select函数可算lambda值，结果给出了3种方法算的的lambda的估计值

## modified HKB estimator is 0.006836982 
## modified L-W estimator is 0.05267247 
## smallest value of GCV  at 0.006 

#以上结果通常取GCV估计，或者观察大多数方法趋近哪个值。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/180759vkgxxkvjtvkozvvx.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;5-lasso&#34;&gt;5. LASSO&lt;/h1&gt;

&lt;p&gt;Tibshirani(1996)提出了Lasso(The Least Absolute Shrinkage and Selectionatoroperator)算法，这里  Absolute 指绝对值。Shrinkage收缩的含义：即系数收缩在一定区域内（比如圆内）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;主要思想&lt;/strong&gt;：
通过构造一个一阶惩罚函数获得一个精炼的模型；通过最终确定一些指标（变量）癿系数为零（岭回归估计系数等于0癿机会微乎其微，造成筛选变量困难），解释力很强。擅长处理具有多重共线性癿数据，筛选变量，与岭回归一样是有偏估计。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/215937llmsx53dfu3fdg6l.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/215611cb0x1eudd6obhi8z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;几何解释&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/220811s52a7l3326a351i3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;由于方框的顶点更容易交于抛物面，也就是lasso更易求解，而该顶点对应的很多系数为0，也就是起到了筛选变量的目的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lasso plot&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(101)
x=matrix(rnorm(1000),100,10)
y=rnorm(100)
fit=glmnet(x,y)

par(mfrow=c(1,3))
#par(mar=c(4.5,4.5,1,4))
##plot1
plot(fit)
vnat=coef(fit)
vnat=vnat[-1,ncol(vnat)] # remove the intercept, and get the coefficients at the end of the path
axis(4, at=vnat,line=-.5,label=paste(&amp;quot;feature&amp;quot;,1:10),las=1,tick=FALSE, cex.axis=0.5)
#plot2
plot(fit, xvar = &amp;quot;lambda&amp;quot;)
# plot3
plot(fit, xvar = &amp;quot;dev&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/lasso201710112315.jpeg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/lasso201710112315.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;6-lasso-vs-岭回归&#34;&gt;6.LASSO vs 岭回归&lt;/h1&gt;

&lt;p&gt;岭回归一方面可以将其变成一个最小二乘问题。另一方面可以将它解释成一个带约束项的系数优化问题。λ增大的过程就是t减小的过程，该图也说明了岭回归系数估计值为什么通常不为0，因为随着抛物面的扩展，它与约束圆的交点可能在圆周上的任意位置，除非交点恰好位于某个坐标轴或坐标平面上，否则大多数情况交点对应的系数值都不为零。再加上λ的选择应使椭球面和圆周的交点恰好在一个坐标平面上，更增加了求解λ的难度。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/222225tr9tt44tn768b45t.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;左图为岭回归，右图为lasso回归。横轴越往左，自由度越小（即圆或方框在收缩的过程），λ越大，系数（即纵轴）会越趋于0。但是岭回归没有系数真正为0，但lasso的不断有系数变为0.&lt;/p&gt;

&lt;h1 id=&#34;7-一般化的模型&#34;&gt;7.一般化的模型&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/222650guccc8yvc8jnxh7j.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;不同q对应的约束域形状&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/222758pwesenw44c4ilds5.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;8-弹性网模型&#34;&gt;8.弹性网模型&lt;/h1&gt;

&lt;p&gt;Zouand Hastie (2005)提出elasticnet，介于岭回归和lasso回归之间，现在被认为是处理多重共线性和变量筛选较好的收缩方法，而且损失的精度不会太多。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/223232tlg6lnnnvlxflhmc.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;9-最小角回归-least-angel-regression-是lasso-regression癿一种高效解法&#34;&gt;9.最小角回归(Least Angel Regression)是lasso regression癿一种高效解法。&lt;/h1&gt;

&lt;p&gt;Lasso回归中表达式用偏导求极值时，存在部分点不可导的情况（如方框的尖点），如何解决？&lt;/p&gt;

&lt;p&gt;Efron于2004年提出癿一种变量选择癿方法，&lt;strong&gt;类似于&lt;/strong&gt;向前逐步回归(Forward Stepwise)的形式，最初用于解决传统的线性回归问题，有清晰的几何意义。&lt;/p&gt;

&lt;p&gt;与向前逐步回归(Forward Stepwise)不同点在于，Forward Step wise 每次都是根据选择的变量子集，完全拟合出线性模型，计算出RSS，再设计统计量（如AIC）对较高癿模型复杂度作出惩罚，而LAR是每次先找出和因变量相关度较高的那个变量, 再沿着LSE的方向一点点调整这个predictor的系数，在这个过程中，这个变量和残差的相关系数会逐渐减小，等到这个相关性没那么显著的时候，就要选进新的相关性较高的变量，然后重新沿着LSE的方向进行变动。而到最后，所有变量都被选中，就和LSE相同了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/103533pkjgm1gjjz0kyj2f.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;左图为LAR逐步加上变量的过程（从左往右看），右图为LASSO变量逐渐淘汰的收缩过程（从右往左看）。
对比两幅图，非常类似。所以可以用LAR方法来计算LASSO，该方法完全是线性解决方法，没有迭代的过程。&lt;/p&gt;

&lt;h1 id=&#34;10-相关系数的几何意义&#34;&gt;10. 相关系数的几何意义&lt;/h1&gt;

&lt;p&gt;设变量y=[y1,y2,&amp;hellip;yn]; 变量x=[x1,x2,&amp;hellip;,xn].&lt;/p&gt;

&lt;p&gt;其相关系数为&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/104719un7jmmy6q0xx6j0z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其中cov&amp;ndash;协方差、var&amp;mdash;方差。&lt;/p&gt;

&lt;p&gt;如果对x和y进行中心化、标准化，则var(y)=var(x)=1,相关系数变为x1y1+x2y2+&amp;hellip;.+xnyn，即为向量x和y的内积=||x||&lt;em&gt;||y||&lt;/em&gt;cos θ，其中θ为x和y的夹角。而对于标准化和中心化后的x和y，则有||x||=||y||=1，所以此时x和y的内积就是它们夹角的余弦。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;如果x和y向量很像，几乎重合，则夹角θ=0，也就是相关系数=内积=1，此时称为高度相关.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果x和y相关程度很低，则表现出来的x和y向量相互垂直，相关系数=0.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果相关系数=-1，标明x和y呈180°，即负相关。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;11-lar算法及几何意义&#34;&gt;11. LAR算法及几何意义&lt;/h1&gt;

&lt;p&gt;参考书The Elements of Statistical Learning .pdf的74页。LAR和Lasso的区别以及LAR解Lasso的修正
参考书The Elements of Statistical Learning .pdf的76页。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/163655y8vuo2dqdu7jo66b.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;假设有6个变量，最先加入与残差向量相关系数较大的浅蓝色v2，在v2变化过程中，相关系数越变越小，直到等于深蓝色的v6，于是加入v6，沿着v2与v6的最小角方向（即向量角分线方向）前进，此后v2和v6与残差向量的相关系数是共同变化的，即两者合并变化，使得相关系数越来越小，直到加入黑色v4为止，三个变量一起变化，&amp;hellip;，一直打到最小二乘解为止，此时残差向量与所有变量的相关系数都为0，即与他们都垂直。&lt;/p&gt;

&lt;p&gt;横坐标L1 Length表示：从原点开始走了多长距离，就是值距离，L1范数。&lt;/p&gt;

&lt;h1 id=&#34;12-r语言中对lar的实现&#34;&gt;12. R语言中对LAR的实现&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;install.packages(&amp;quot;lars&amp;quot;)  #lars包
longley  #用longley数据集，它是一个著名的多重共线性例子
w=as.matrix(longley)  #将数据集转换为一个矩阵

laa=lars(w[,2:7],w[,1]) #w的2:7列为自变量，第1列为因变量
laa  #显示LAR回归过程

##Call:
##lars(x = w[, 2:7], y = w[, 1])
##R-squared: 0.993 
##Sequence of LASSO moves:
##     GNP Year Armed.Forces Unemployed Employed Population Year Employed   Employed Year Employed Employed
##Var    1    5            3                   2                   6           4          -5           -6            6             5  -6        6                                                 
##Step  1    2            3                   4                   5           6           7          8             9             10  11       12 


plot(laa)  #画lasso回归过程图
summary(laa)

#以上结果显示了每一步的残差平方和RSS和多重共线性指标Cp（Mallows&#39;s Cp http://en.wikipedia.org/wiki/Mallows%27_Cp）
#Cp越小，多重共线性越小，因此结果以第八步为准，即只剩下第1、2、3、4个变量
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;13-glmnet包&#34;&gt;13.glmnet包&lt;/h1&gt;

&lt;p&gt;From &lt;a href=&#34;https://site.douban.com/182577/widget/notes/10567212/note/289294468/&#34;&gt;https://site.douban.com/182577/widget/notes/10567212/note/289294468/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;glmnet包是关于Lasso and elastic-net regularized generalized linear models。 作者是Friedman, J., Hastie, T. and Tibshirani, R这三位。&lt;/p&gt;

&lt;p&gt;这个包采用的算法是循环坐标下降法（cyclical coordinate descent），处理的模型包括 linear regression,logistic and multinomial regression models, poisson regression 和 the Cox model，用到的正则化方法就是l1范数（lasso）、l2范数（岭回归）和它们的混合 （elastic net）。&lt;/p&gt;

&lt;p&gt;坐标下降法是关于lasso的一种快速计算方法（是目前关于lasso最快的计算方法），其基本要点为： 对每一个参数在保持其它参数固定的情况下进行优化，循环，直到系数稳定为止。这个计算是在lambda的格点值上进行的。 关于这个算法见[5]。 关于glmnet包的细节可参考[4]，这篇文献同时也是关于lasso的一个不错的文献导读。&lt;/p&gt;

&lt;p&gt;[1]Tibshirani, R.: Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society: Series B, Vol. 58 (1996), No 1, 267–288&lt;/p&gt;

&lt;p&gt;[2]Efron, B., Johnstone, I., Hastie, T., and Tibshirani, R.: Least angle regression. Annals of Statistics, Vol. 32 (2004), No 2, 407–499.&lt;/p&gt;

&lt;p&gt;[3]Hastie, T., Tibshirani, R., and Friedman, J.: The Elements of Statistical Learning: Data Mining, Inference and Prediction. Second edition. New York: Springer, 2009.&lt;/p&gt;

&lt;p&gt;[4]Friedman,J.,Hastie,T.,Tibshirani.R.:Regularization Paths for Generalized Linear Models via Coordinate Descent.Journal of Statistical Software,Volume 33(2010), Issue 1.&lt;/p&gt;

&lt;p&gt;[5]J. Friedman, T. Hastie, H. Hoe ing, and R. Tibshirani.:Pathwise coordinate optimization. Annals of Applied Statistics, 2(1):302-332, 2007. &lt;a href=&#34;http://www.stanford.edu/~hastie/Papers/pathwise.pdf&#34;&gt;http://www.stanford.edu/~hastie/Papers/pathwise.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6]Trevor Hastie,Sparse Linear Models:with demonstrations using glmnet.2013.&lt;/p&gt;

&lt;p&gt;[7] Zou, Hui &amp;amp; Trevor Hastie (2005): Regularization and variable selection via the Elastic Net, JRSS (B)67(2):301-320)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(glmnet)
prostate=read.csv(url(&amp;quot;https://taoshengxu.github.io/DocumentGit/data/prostate.csv&amp;quot;))
prostate=prostate[,c(1,3,4,6,7,9)]
head(prostate)
x &amp;lt;- as.matrix(prostate[, 2:6])
y &amp;lt;- prostate[, 1]
set.seed(1)
train &amp;lt;- sample(1:nrow(x), nrow(x) * 2/3)
test &amp;lt;- (-train)

## 1. Ridge Regression
r1 &amp;lt;- glmnet(x = x[train, ], y = y[train], family = &amp;quot;gaussian&amp;quot;, alpha = 0)
plot(r1, xvar = &amp;quot;lambda&amp;quot;)

r1.cv &amp;lt;- cv.glmnet(x = x, y = y, family = &amp;quot;gaussian&amp;quot;, alpha = 0, nfold = 10)
plot(r1.cv)

mte &amp;lt;- predict(r1, x[test, ])
mte &amp;lt;- apply((mte - y[test])^2, 2, mean)
points(log(r1$lambda), mte, col = &amp;quot;blue&amp;quot;, pch = 19)
legend(&amp;quot;topleft&amp;quot;, legend = c(&amp;quot;10 - fold CV&amp;quot;, &amp;quot;Test&amp;quot;), col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;))

r1.min &amp;lt;- glmnet(x = x, y = y, family = &amp;quot;gaussian&amp;quot;, alpha = 0, lambda = r1.cv$lambda.min)
coef(r1.min)

##2. Lasso

r2 &amp;lt;- glmnet(x = x[train, ], y = y[train], family = &amp;quot;gaussian&amp;quot;, alpha = 1)
plot(r2)
plot(r2, xvar = &amp;quot;lambda&amp;quot;)

r2.cv &amp;lt;- cv.glmnet(x = x, y = y, family = &amp;quot;gaussian&amp;quot;, alpha = 1, nfold = 10)
plot(r2.cv)

mte &amp;lt;- predict(r2, x[test, ])
mte &amp;lt;- apply((mte - y[test])^2, 2, mean)
points(log(r2$lambda), mte, col = &amp;quot;blue&amp;quot;, pch = 19)
legend(&amp;quot;topleft&amp;quot;, legend = c(&amp;quot;10 - fold CV&amp;quot;, &amp;quot;Test&amp;quot;), col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;))

# cv.min vs cv.1se,用全部数据再次拟合模型
r2.cv$lambda.min
## [1] 0.002954
r2.cv$lambda.1se
## [1] 0.1771

r2.1se &amp;lt;- glmnet(x = x, y = y, family = &amp;quot;gaussian&amp;quot;, alpha = 1, lambda = r2.cv$lambda.1se)
coef(r2.1se)
## 6 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
## s0
## (Intercept) 0.3234
## age . 
## lbph . 
## lcp 0.2462
## gleason . 
## lpsa 0.4320
r2.min &amp;lt;- glmnet(x = x, y = y, family = &amp;quot;gaussian&amp;quot;, alpha = 1, lambda = r2.cv$lambda.min)
coef(r2.min)
## 6 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
## s0
## (Intercept) -1.44505
## age 0.01851
## lbph -0.08585
## lcp 0.29688
## gleason 0.05081
## lpsa 0.53741

# 岭回归和lasso的比较
lasso.pred &amp;lt;- predict(r2, s = r2.cv$lambda.1se, newx = x[test, ])
ridge.pred &amp;lt;- predict(r1, s = r1.cv$lambda.1se, newx = x[test, ])
mean((lasso.pred - y[test])^2)
## [1] 0.3946
mean((ridge.pred - y[test])^2)
## [1] 0.4239

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;关于glmnet包的使用&#34;&gt;关于glmnet包的使用&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;(1)glment（）和cv.glmnet()&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;第一次用这个包的时候，我有个很蠢的问题，为什么有了cv.glmnet()还需要保留glmnet（）呢？ cv.glmnet()可以通过交叉验证得到（关于lambda的）最优的方程，但是就glment包来说仍然不是一个完美的结果，关于alpha的交叉验证依然需要使用者自己来完成（包的文档中给了点提示）。glmnet（）仍然需要保留，因为可以得到正则化的路径，因为算法的原因，coordinate descent 在选取极值上有随机性，路径在变量的选择中还是很重要的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(2)cv.glmnet() 中的lambda.min和lambda.1se&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;lambda.min:   value of lambda that gives minimum cvm.&lt;/p&gt;

&lt;p&gt;lambda.1se:   largest value of lambda such that error is within 1 standard error of the minimum.&lt;/p&gt;

&lt;p&gt;关于这两个输出值的使用，似乎有点混乱。看了很多网上的讨论推荐使用lambda.1se的比较多，这样可以得到更简洁的模型。 涉及到所谓的1-SE rule。 “one standard error” rule to select the best model, i.e. selecting the most parsimonious model from the subset of models whose score is within one standard error of the best score.但是还有这样的说法：1se rule在低noise的时候才好用高noise的时候，有一两个fold的error很大，cv curve就会增长很快，导致选的lambda太大。&lt;/p&gt;

&lt;h1 id=&#34;14-glmnet-vignettes-非常易读-有益理解&#34;&gt;14.glmnet Vignettes 非常易读，有益理解&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf&#34;&gt;https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/glmnet/vignettes/Coxnet.pdf&#34;&gt;https://cran.r-project.org/web/packages/glmnet/vignettes/Coxnet.pdf&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Cox 分层原理</title>
      <link>/blog/cn/2017/09/coxstratified/</link>
      <pubDate>Wed, 13 Sep 2017 12:43:10 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/coxstratified/</guid>
      <description>
        

&lt;p&gt;&lt;a href=&#34;https://taoshengxu.github.io/DocumentGit/pdf/Cox+Stratified.pdf&#34;&gt;PPT&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;首先理解为什么cox模型会对协变量分层处理&#34;&gt;首先理解为什么COX模型会对协变量分层处理？&lt;/h1&gt;

&lt;p&gt;需要分层的变量不满足PH假设，需要分层处理。&lt;/p&gt;

&lt;h1 id=&#34;如何确定-协变量不满足ph假设&#34;&gt;如何确定 协变量不满足PH假设？&lt;/h1&gt;

&lt;p&gt;首先对需要研究的协变量进行多协变量COX回归，挑出不满足PH假设的协变量&lt;/p&gt;

&lt;h1 id=&#34;cross-validated-partial-likelihood-cvpl-for-the-cox-model&#34;&gt;Cross-validated partial likelihood (CVPL) for the Cox model&lt;/h1&gt;

&lt;p&gt;cvpl {in Package survcomp} function&lt;/p&gt;

&lt;h1 id=&#34;toc_3&#34;&gt;&amp;hellip;&lt;/h1&gt;

&lt;p&gt;如果得到一个Subtypes 信息，对subtype分层进行 协变量为 age的COX 回归。目的是研究在不同亚型内，age 是否为影响生存预后的重要因素。&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>混淆矩阵（confusion matrix）</title>
      <link>/blog/cn/2017/09/confusionmatrix/</link>
      <pubDate>Sun, 10 Sep 2017 03:11:14 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/confusionmatrix/</guid>
      <description>
        &lt;p&gt;在学习&lt;a href=&#34;http://bioconductor.org/packages/release/bioc/vignettes/genefu/inst/doc/genefu.pdf&#34;&gt;genefu包&lt;/a&gt;时候遇到Confusion Matrix，有必要有个系统的学习和总结。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;混淆矩阵（confusion matrix）是可视化工具，特别用于监督学习， &lt;em&gt;在无监督学习一般叫做匹配矩阵&lt;/em&gt; 。在图像精度评价中，主要用于比较分类结果和实际测得值，可以把分类结果的精度显示在一个混淆矩阵里面。混淆矩阵是通过将每个实测像元的位置和分类与分类图像中的相应位置和分类像比较计算的。&lt;/p&gt;

&lt;p&gt;混淆矩阵的每一列代表了预测类别[1]  ，每一列的总数表示预测为该类别的数据的数目；每一行代表了数据的真实归属类别[1]  ，每一行的数据总数表示该类别的数据实例的数目。每一列中的数值表示真实数据被预测为该类的数目：如下图，第一行第一列中的43表示有43个实际归属第一类的实例被预测为第一类，同理，第二行第一列的2表示有2个实际归属为第二类的实例被错误预测为第一类。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;如有150个样本数据，这些数据分成3类，每类50个。分类结束后得到的混淆矩阵为：
                   预测
              类1 类2 类3
      类1     43   5   2
实际  类2      2   45  3
      类3     0    1   49
每一行之和为50，表示50个样本，
第一行说明类1的50个样本有43个分类正确，5个错分为类2，2个错分为类3
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;另外一个例子 From:&lt;a href=&#34;http://blog.csdn.net/vesper305/article/details/44927047&#34;&gt;http://blog.csdn.net/vesper305/article/details/44927047&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;假设有一个用来对猫（cats）、狗（dogs）、兔子（rabbits）进行分类的系统，混淆矩阵就是为了进一步分析性能而对该算法测试结果做出的总结。假设总共有 27 只动物：8只猫， 6条狗， 13只兔子。结果的混淆矩阵如下图：
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/confusionmatrix0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在这个混淆矩阵中，实际有 8只猫，但是系统将其中3只预测成了狗；对于 6条狗，其中有 1条被预测成了兔子，2条被预测成了猫。从混淆矩阵中我们可以看出系统对于区分猫和狗存在一些问题，但是区分兔子和其他动物的效果还是不错的。所有正确的预测结果都在对角线上，所以从混淆矩阵中可以很方便直观的看出哪里有错误，因为他们呈现在对角线外面。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/confusionmatrix1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在预测分析中，混淆表格（有时候也称为混淆矩阵），是由false positives，false negatives，true positives和true negatives组成的两行两列的表格。它允许我们做出更多的分析，而不仅仅是局限在正确率。准确率对于分类器的性能分析来说，并不是一个很好地衡量指标，因为如果数据集不平衡（每一类的数据样本数量相差太大），很可能会出现误导性的结果。例如，如果在一个数据集中有95只猫，但是只有5条狗，那么某些分类器很可能偏向于将所有的样本预测成猫。整体准确率为95%，但是实际上该分类器对猫的识别率是100%，而对狗的识别率是0%。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;下面内容总结了假设检验的重要内容，清晰全面。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/confusionmatrix2.png&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/confusionmatrix3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Fold Change 与火山图</title>
      <link>/blog/cn/2017/09/foldchange/</link>
      <pubDate>Sat, 09 Sep 2017 10:19:35 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/foldchange/</guid>
      <description>
        

&lt;h1 id=&#34;fold-change&#34;&gt;Fold Change&lt;/h1&gt;

&lt;p&gt;计算公式：样本平均值log2还原的比值&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/fold_change.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;火山图-volcano-plot&#34;&gt;火山图（Volcano Plot）&lt;/h1&gt;

&lt;p&gt;火山图只存在于两分组样本比较中，并且有生物学重复(经过相同方式处理的相同样品)
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/volcanoplot.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;标准的火山图常用于展示显著差异表达的基因，这里有两个关键词：显著是指P&amp;lt;&amp;gt;差异表达一般我们按照Fold Change(倍数变化)&amp;gt;=2.0作为标准。&lt;/p&gt;

&lt;p&gt;当我们拿到基因表达的P值和倍数后，为了用火山图展示结果，一般需要把倍数进行Log2的转化，比如某基因在实验组表达水平是对照组的4倍，log2（4）=2，同样的如果是1/4，也就是0.25，转换后的结果就是-2。&lt;/p&gt;

&lt;p&gt;同样的道理，对P值进行-log10的转化，-log10（0.05）约等于1.30103，由于P值越小表示越显著，所以我们进行-log10（P value）转化后，转化值越大表示差异约显著，比如-log10（0.001）=3  &amp;gt;  -log10(0.01)=2 &amp;gt;  -log10(0.05)=1.30。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/volcanoplot1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;在上面这个图中，横轴是log2（FC），纵轴是-log10（P value），每个点代表一个基因，平行于Y轴的两条线分别是X=1和X=-1，在X=-1左侧的点是下调2倍以上的基因，在X=1右侧的点是上调2倍以上的基因。同时，平行于X轴有一条虚线Y=1.30，即-log10(0.05），在虚线以上的点表示显著性.&lt;/p&gt;

&lt;p&gt;这样，我们就把虚线Y=1.30以上，X=1右侧和X=-1左侧的基因标记为表达显著差异的基因，一般我们把大于2倍（X=1右侧）的点标记为红色，把小于-2（X=-1左侧）的点标记为绿色，一些我们特别关注的基因需要把基因名标记出来。&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>GCBI 学院</title>
      <link>/blog/cn/2017/09/gcbiaccdemy/</link>
      <pubDate>Sat, 09 Sep 2017 09:19:35 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/gcbiaccdemy/</guid>
      <description>
        &lt;p&gt;GCBI学院里一些视频还是值得看看的,一直想看看一直忘记，记录在这里。 &lt;a href=&#34;http://college.gcbi.com.cn/&#34;&gt;Here&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>ExpressionSet简单讲解</title>
      <link>/blog/cn/2017/09/expressionset/</link>
      <pubDate>Fri, 08 Sep 2017 16:38:17 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/expressionset/</guid>
      <description>
        

&lt;p&gt;From 生信菜鸟团&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
&lt;p&gt;这个对象其实是对表达矩阵加上样本分组信息的一个封装，由biobase这个包引入。它是eSet这个对象的继承。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;一个现成例子&#34;&gt;一个现成例子&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;下面是一个具体的例子，来源于CLL这个包，是用hgu95av2芯片测了22个样本&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;    library(CLL)
    data(sCLLex)
    sCLLex
    
    ExpressionSet (storageMode: lockedEnvironment)
    assayData: 12625 features, 22 samples  ##表达矩阵
      element names: exprs 
    protocolData: none
    phenoData
      sampleNames: CLL11.CEL CLL12.CEL ... CLL9.CEL (22 total)
      varLabels: SampleID Disease   ## 样本分组信息
      varMetadata: labelDescription
    featureData: none
    experimentData: use &#39;experimentData(object)&#39;
    Annotation: hgu95av2
    
    &amp;gt; exprMatrix=exprs(sCLLex)
    &amp;gt; dim(exprMatrix)
    [1] 12625    22
    &amp;gt; meta=pData(sCLLex)
    &amp;gt; table(meta$Disease)
    
    progres.   stable 
          14        8 
    &amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;根据上面的信息可以看出该芯片共12625个探针，这22个样本根据疾病状态分成两组，14vs8
这个数据对象就可以打包做很多包的分析输入数据。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对这个包的分析，重点就是 &lt;strong&gt;&lt;code&gt;exprs&lt;/code&gt; 函数提取表达矩阵&lt;/strong&gt;，&lt;strong&gt;&lt;code&gt;pData&lt;/code&gt; 函数看看该对象的样本分组信息&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;limma等包使用该对象作为输入数据&#34;&gt;limma等包使用该对象作为输入数据&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;下面这个例子充分说明了 &lt;code&gt;ExpressionSet&lt;/code&gt; 对象的重要性&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;    &amp;gt; library(limma)
    &amp;gt; design=model.matrix(~factor(sCLLex$Disease))
    &amp;gt; fit=lmFit(sCLLex,design)
    &amp;gt; fit=eBayes(fit)
    &amp;gt; options(digits = 4)
    &amp;gt; topTable(fit,coef=2,adjust=&#39;BH&#39;)
               logFC AveExpr      t   P.Value adj.P.Val     B
    39400_at  1.0285   5.621  5.836 8.341e-06   0.03344 3.234
    36131_at -0.9888   9.954 -5.772 9.668e-06   0.03344 3.117
    33791_at -1.8302   6.951 -5.736 1.049e-05   0.03344 3.052
    1303_at   1.3836   4.463  5.732 1.060e-05   0.03344 3.044
    36122_at -0.7801   7.260 -5.141 4.206e-05   0.10619 1.935
    36939_at -2.5472   6.915 -5.038 5.362e-05   0.11283 1.737
    41398_at  0.5187   7.602  4.879 7.824e-05   0.11520 1.428
    32599_at  0.8544   5.746  4.859 8.207e-05   0.11520 1.389
    36129_at  0.9161   8.209  4.859 8.212e-05   0.11520 1.389
    37636_at -1.6868   5.697 -4.804 9.355e-05   0.11811 1.282
    &amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;还有非常多的其它包会使用 &lt;code&gt;ExpressionSet&lt;/code&gt; 对象，我就不一一介绍了。&lt;/p&gt;

&lt;h2 id=&#34;自己构造-expressionset-对象&#34;&gt;自己构造 &lt;code&gt;ExpressionSet&lt;/code&gt; 对象&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;根据上面的讲解，我们知道了在这个对象其实很简单，就是对表达矩阵加上样本分组信息的一个封装。
所以我们就用上面得到的exprMatrix和meta来构建一个ExpressionSet对象，biobase包里面提供了详细的说明,建议大家仔细看官方手册&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;    metadata &amp;lt;- data.frame(labelDescription=c(&#39;SampleID&#39;, &#39;Disease&#39;),
                       row.names=c(&#39;SampleID&#39;, &#39;Disease&#39;))
    phenoData &amp;lt;- new(&amp;quot;AnnotatedDataFrame&amp;quot;,data=meta,varMetadata=metadata)
    myExpressionSet &amp;lt;- ExpressionSet(assayData=exprMatrix,
                                     phenoData=phenoData,
                                     annotation=&amp;quot;hgu95av2&amp;quot;)
    &amp;gt; myExpressionSet
    ExpressionSet (storageMode: lockedEnvironment)
    assayData: 12625 features, 22 samples 
      element names: exprs 
    protocolData: none
    phenoData
      sampleNames: CLL11.CEL CLL12.CEL ... CLL9.CEL (22 total)
      varLabels: SampleID Disease
      varMetadata: labelDescription
    featureData: none
    experimentData: use &#39;experimentData(object)&#39;
    Annotation: hgu95av2 
    &amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;从上面的构造过程可以看出，重点就是表达矩阵加上样本分组信息&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;其它例子&#34;&gt;其它例子&lt;/h2&gt;

&lt;h3 id=&#34;all包的数据自带-expressionset-对象&#34;&gt;ALL包的数据自带 &lt;code&gt;ExpressionSet&lt;/code&gt; 对象&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;    library(ALL)
    data(ALL)
    ALL
    
    ExpressionSet (storageMode: lockedEnvironment)
    assayData: 12625 features, 128 samples
        element names: exprs
    protocolData: none
    phenoData
        sampleNames: 01005 01010 … LAL4 (128 total)
        varLabels: cod diagnosis … date last seen (21 total)
        varMetadata: labelDescription
    featureData: none
    experimentData: use ‘experimentData(object)’
    pubMedIds: 14684422 16243790 
    Annotation: hgu95av2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个数据非常出名，很多其它算法包都会拿这个数据来举例子，只有真正理解了ExpressionSet对象才能学会bioconductor系列包&lt;/p&gt;

&lt;h2 id=&#34;用geoquery包来下载得到-expressionset-对象&#34;&gt;用GEOquery包来下载得到 &lt;code&gt;ExpressionSet&lt;/code&gt; 对象&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;    gse1009=GEOquery::getGEO(&amp;quot;GSE1009&amp;quot;)
    gse1009[[1]] ## 这就是ExpressionSet对象
&lt;/code&gt;&lt;/pre&gt;

        
      </description>
    </item>
    
    <item>
      <title>生存曲线美化</title>
      <link>/blog/cn/2017/09/%E7%94%9F%E5%AD%98%E6%9B%B2%E7%BA%BF%E7%BE%8E%E5%8C%96/</link>
      <pubDate>Thu, 07 Sep 2017 16:27:18 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/%E7%94%9F%E5%AD%98%E6%9B%B2%E7%BA%BF%E7%BE%8E%E5%8C%96/</guid>
      <description>
        &lt;p&gt;好久以前在微信里看到这个文章。今天整理用rmarkdonw整理在这里，以后更新的我的包里。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzIyMDUwOTQwNA==&amp;amp;mid=2247483665&amp;amp;idx=1&amp;amp;sn=05469909bf70e234fb71a3653e51ba8b&amp;amp;scene=23&amp;amp;srcid=072848z8s1DucjVqpZDqAYTu#rd&#34;&gt;原文&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(survival)
library(ggplot2)
library(survminer)
# 载入数据
#使用Surv（）函数建立基本生存对象
fit&amp;lt;- survfit(Surv(time, status) ~ sex, data = lung)
summary(fit) #查看结果
#使用survminer程序包ggsurvplot（）函数绘制生存曲线
#简单绘图
ggsurvplot(fit)
#分生存曲线下面给出number.at risk
ggsurvplot(fit,risk.table=TRUE)
#添加log-rank检验p-value
ggsurvplot(fit,risk.table=TRUE,pval=TRUE)
#添加置信区间带
ggsurvplot(fit,risk.table=TRUE,conf.int=TRUE,pval=TRUE)
&lt;/code&gt;&lt;/pre&gt;

        
      </description>
    </item>
    
    <item>
      <title>GSEA 参悟</title>
      <link>/blog/cn/2017/09/gsea/</link>
      <pubDate>Thu, 07 Sep 2017 14:56:07 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/gsea/</guid>
      <description>
        

&lt;p&gt;GSEA（Gene Set Enrichment Analysis）方法是目前在pathway analysis方法中，归类为functional score analysis裡的state of the art.&lt;/p&gt;

&lt;p&gt;还是没搞明白，下次再找好资料研究一下。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;gsea结果理解&#34;&gt;GSEA结果理解&lt;/h1&gt;

&lt;p&gt;中间从蓝色到红色的过渡“带”表示基因从上调到下调排列（排序可以按照fold change,也可以是p-value)。黑色像条形码的竖线表示该位置的基因属于某个指定通路。绿色有波动的曲线表示富集分数，从0开始计算，属于基因通路增加，不属于则减少。最后看下黑色的条形码是不是富集在一端。&lt;/p&gt;

&lt;p&gt;作者：hoptop
链接：&lt;a href=&#34;http://www.jianshu.com/p/199b44974480&#34;&gt;http://www.jianshu.com/p/199b44974480&lt;/a&gt;
來源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/GSEA.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;首先，可以设定一个衡量差异表达程度的统计量，这里简单起见，用log Fold Change，来把基因排序。上面那个颜色条就表示一共有17425个基因表达，下方数字表示该处对应的基因所在的序号，红色到蓝色，表示从上调到下调。黑色的杠杠，表示在该位置处的基因属于Myc靶基因，那一共就是有188个杠杠。颜色条上方有个数字9109，它表示这这里，基因表达从上调转变成下调。那个位置颜色是白色的，也就是说，倍数差异接近0了。
在GSEA这个检验里面，我们实际上就是在检验上面颜色条里黑色的杠杠，是否有往颜色条一端富集的趋势。
实际在做这个检验时，我们是从红色的序号为1的基因出发到蓝色的序号为17425的基因，这个过程中，遍历每一个基因，每次都查看下当前基因是否是Myc靶基因，如果是，则累加一点分数，否，则扣掉一点分数。这个分数的轨迹，也就是上图中的深绿色曲线。
接下来的问题是，如何得到统计显著性？统计假设检验的本质就是先生成一个零假设的数据分布，然后观察实际数据在这个零假设分布下，是不是在尾端。好了，我们把这句话具现到我们这个GSEA的例子中来。我们有三种方式：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;颜色条不动它，把黑色的杠杠，从颜色条上拿起来，然后再随机的放到颜色条上&lt;/li&gt;
&lt;li&gt;把样本的分组打乱，随机分组，重新计算排序统计量，然后排序&lt;/li&gt;
&lt;li&gt;即做1，又做2&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第一种方式，对算力要求最低，对样本容量没有要求，但是不考虑基因间的相关性，可能导致一定的假阳性。第二种方式，对算力要求较高，要求一定的样本容量（每组重复数）以保证有效置换次数，可以保持基因间的协方差结构，但power会略低。第三种方式，算力要求最高。
这里假设我们随便选一种方式，重排一次以后，可以按照原先绘制绿色曲线的方法绘制一条新的曲线（零假设的数据），重复这个过程千万次的话，就可以比较精确的得到零假设的覆盖区域了，求取这个阴影的第5到95百分位数的区间，即可绘出结果图中的浅灰色阴影了。
这样统计检验的显著性，就可视化成为观察绿色曲线与灰色阴影的偏离程度了。绿色曲线离x轴最大的偏离值即为该检验的Enrichment Score (ES)，把它对Myc靶基因的数量再校正一下，就可以得到 Normalized Enrichment Score (NES)。这里我们看下结果，非常显著，这个节奏和刚才用Fisher &amp;rsquo;s exact test的结果，明显不一样，这又是为什么呢？
请仔细观察GSEA结果图里，排序统计量和颜色条上黑杠杠的分布。可以发现，绝大部分的Myc靶基因，分布在浅蓝色区域，即绝大部分Myc靶基因都是下调，但是只是微弱的下调，所以它们没有在Fisher exact test中被计入为差异基因。&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>基因本体论(Gene Ontology) 与通路分析(Pathway Analysis)</title>
      <link>/blog/cn/2017/09/ontology/</link>
      <pubDate>Thu, 07 Sep 2017 12:06:46 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/ontology/</guid>
      <description>
        

&lt;p&gt;&lt;a href=&#34;https://bioconductor.org/packages/release/bioc/vignettes/clusterProfiler/inst/doc/clusterProfiler.html&#34;&gt;clusterProfiler&lt;/a&gt;包的很多内容还是理解不了，需要找人问问。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Q1: GO的level，在什么研究中会涉及到？ 一般来说，level越大，GO功能越具体。level就像一棵树树主干一样，发了不同的枝叶，level越大枝叶越详细。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Q2： GO over-representation test 是我们最经常用到的基因功能分析，为什么没有考虑到level的问题，一般选择哪种（CC, MF,BP）进行分析？&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Q3： GO Gene Set Enrichment Analysis 大概的原理我能明白，结果图如阿理解？&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;基因本体论&#34;&gt;基因本体论&lt;/h1&gt;

&lt;p&gt;针对于单个基因特征&lt;/p&gt;

&lt;p&gt;本体论这个词一看就逼格很高的样子，源于哲学，本体论用于描述事物的本质，所以基因本体论就是为了描述基因的本质。GO从三个方面对基因的本质进行描述，&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1）细胞组分（Cellular Component, CC）：一般用来描述基因作用的位置，比如说高尔基体，内质网这样的；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2）分子功能（Molecular Function, MF）：可以描述为分子水平的活性，如催化或结合活性；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;3）生物学过程(BP)：比如说蛋白质磷酸化，细胞粘附都是生物学过程。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单地说，GO就像是给基因贴标签进行注释，比如说给X湿兄贴标签，出没地点——小张聊科研(CC)，文风诙谐幽默(MF)，能够让大家轻松愉悦地学到东西(BP)。&lt;/p&gt;

&lt;p&gt;GO的术语是分层的，呈现出树状结构，上文提到的CC、MF和BP即为GO术语的最顶层，比如说下图是BP的分析结果树状图，最顶端即为BP.
&lt;img src=&#34;http://img.mp.itc.cn/upload/20170419/a6c07195ca0f4b738699979d31fbbeb2_th.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;通路分析&#34;&gt;通路分析&lt;/h1&gt;

&lt;p&gt;针对于群基因（protein）特征&lt;/p&gt;

&lt;p&gt;一个生物学过程的实现会涉及到许多蛋白质，这些蛋白质合在一起就是一个通路。通路分析能够帮助我们更好地了解某个或某一些蛋白质在一个生物学过程中所扮演的角色。通路分析和GO都是对基因进行注释，那么为什么要对基因进行注释呢？因为基因说穿了其实是一串RNA，那么它的功能和结构虽然都是客观存在的，但是要如何描述这些客观的东西是基因注释所要解决的问题。
最常用的通路分析数据库是京都基因与基因组百科全书 (Kyoto Encyclopedia of Genes and Genomes, KEGG)。1995年，KEGG数据库项目由京都大学化学研究所教授Minoru Kanehisa领头启动。KEGG数据库是手工绘制的KEGG途径图的集合，每个途径图包含分子相互作用和反应的网络，将基因组中的基因与通路中的基因产物（主要是蛋白质）连接。KEGG pathway analysis即为将目的基因定位到KEGG途径图中的过程。下图为small cell lung cancer的KEGG途径图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.mp.itc.cn/upload/20170419/7f13e82686194a129b65cfc5a0403d7b_th.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;go分析-与-pathyway-分析-总结&#34;&gt;GO分析 与 Pathyway 分析 总结&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;GO数据库分别从功能、参与的生物途径及细胞中的定位对基因产物进行了标准化描述，即对基因产物进行简单注释，通过GO富集分析可以粗略了解差异基因富集在哪些生物学功能、途径或者细胞定位。GO分析好比是将基因分门别类放入一个个功能类群的篮子，而pathway则是将基因一个个具体放到代谢网络中的指定位置。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pathway指代谢通路，对差异基因进行pathway分析，可以了解实验条件下显著改变的代谢通路，在机制研究中显得尤为重要。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;基因功能分析-gene-ontology-和代谢通路-pathway-分析方法-核心&#34;&gt;基因功能分析(Gene Ontology)和代谢通路（pathway）分析方法（核心）&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;进行样本组和对照组基因表达差异分析&lt;/li&gt;
&lt;li&gt;对获取的差异表达基因进行功能（GO）和信号通路（Pathway）分析&lt;/li&gt;
&lt;li&gt;在得到功能（GO）和信号通路（Pathway）分析的结果中找出和疾病/研究目标相关的GO 和 Pathway&lt;/li&gt;
&lt;li&gt;对这些相关的基因功能和信号通路的基因取交集，缩小候选基因的范围。&lt;/li&gt;
&lt;li&gt;对取交集得到的基因，如果基因数目还比较多（目标基因：1-2个），就将这些基因和差异表达基因再取交集，根据Fold Change 选择差异表达倍数最大的基因作为我们研究的候选基因。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/go_pathway0.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/go_pathway0-1.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/go_pathway1.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/go_pathway2.jpg&#34; alt=&#34;&#34; /&gt;
&lt;img src=&#34;https://taoshengxu.github.io/DocumentGit/img/go_pathway3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;date: &amp;lsquo;2016-12-15 17:07:42&amp;rsquo;&lt;/p&gt;

&lt;p&gt;干Bioinformatics差不多2年半了，却一直到对GO和pathway的区别搞不清楚，现在明白又觉好笑，记几个字在这里。&lt;/p&gt;

&lt;p&gt;一般对一组Gene Set 做GO是想看哪些生物功能；比如一个功能，有很多基因都和这个功能相关，把这个功能相关的所有基因找出来。&lt;/p&gt;

&lt;p&gt;而对一组GeneSet 做Kegg主要是看通路，在通路上的联系。&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Git 参考手册</title>
      <link>/blog/cn/2017/09/git_command/</link>
      <pubDate>Thu, 07 Sep 2017 09:06:46 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/git_command/</guid>
      <description>
        &lt;p&gt;Git &lt;a href=&#34;http://gitref.justjavac.com/&#34;&gt;参考手册中文&lt;/a&gt; &lt;br&gt;
Git &lt;a href=&#34;https://git-scm.com/about&#34;&gt;参考手册ENG&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;git check out -b 创建新分支，并立即切换到它。与以下等效：

&lt;ul&gt;
&lt;li&gt;git branch newbranch&lt;/li&gt;
&lt;li&gt;git checkout newbranch&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;git merge [branch] 将[branch] 分枝合并到当前分支中&lt;/li&gt;
&lt;li&gt;git push [alias] [branch]，就会将你的[branch]分支推送成为[alias]远端上的[branch] 分支&lt;/li&gt;
&lt;/ul&gt;

        
      </description>
    </item>
    
    <item>
      <title>HTML基本语法</title>
      <link>/blog/cn/2017/09/html_basic/</link>
      <pubDate>Tue, 05 Sep 2017 23:03:13 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/html_basic/</guid>
      <description>
        &lt;p&gt;wait for&amp;hellip;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Ubuntu 安装R</title>
      <link>/blog/cn/2017/09/ubuntu_r/</link>
      <pubDate>Tue, 05 Sep 2017 23:03:13 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/ubuntu_r/</guid>
      <description>
        

&lt;ol&gt;
&lt;li&gt;Ubuntu16.04&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;r&#34;&gt;R&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sudo echo &amp;quot;deb http://cran.rstudio.com/bin/linux/ubuntu xenial/&amp;quot; | sudo tee -a /etc/apt/sources.list 
gpg --keyserver keyserver.ubuntu.com --recv-key 51716619E084DAB9
gpg -a --export 51716619E084DAB9 | sudo apt-key add -
sudo apt-get update
sudo apt-get install r-base r-base-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;rstudio&#34;&gt;Rstudio&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34;&gt;Latest file&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sudo apt-get install gdebi-core
sudo gdebi -n rstudio-1.0.44-amd64.deb
rm rstudio-1.0.44-amd64.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;rstudio-server&#34;&gt;Rstudio server&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.rstudio.com/products/rstudio/download-server/&#34;&gt;Latest&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sudo apt-get install gdebi-core
sudo gdebi -n rstudio-1.0.44-amd64.deb
rm rstudio-1.0.44-amd64.deb
完成安装后，RStudio Server会自动启动运行
ps -aux|grep rstudio
8787端口被打开
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;root用户无法登陆，新建一个用户进行登陆 
useradd -d /home/R -m R，创建用户的同时指定主目录 
passwd R，设置密码

系统设置 
主要有两个配置文件，默认文件不存在 
/etc/rstudio/rserver.conf 
/etc/rstudio/rsession.conf

设置端口和ip控制:
vi /etc/rstudio/rserver.conf
www-port=8080#监听端口
www-address=127.0.0.0#允许访问的IP地址，默认0.0.0.0
重启服务器，生效
rstudio-server restart

会话配置管理
vi /etc/rstudio/rsession.conf
session-timeout-minutes=30#会话超时时间
r-cran-repos=http://ftp.ctex.org/mirrors/CRAN#CRAN资源库

rstudio-server start #启动
rstudio-server stop #停止
rstudio-server restart #重启

查看运行中R进程
rstudio-server active-sessions
指定PID，停止运行中的R进程
rstudio-server suspend-session &amp;lt;pid&amp;gt;
停止所有运行中的R进程
rstudio-server  suspend-all
强制停止运行中的R进程，优先级最高，立刻执行
rstudio-server force-suspend-session &amp;lt;pid&amp;gt;
rstudio-server force-suspend-all
RStudio Server临时下线，不允许web访问，并给用户友好提示
rstudio-server offline
RStudio Server临时上线
rstudio-server online
&lt;/code&gt;&lt;/pre&gt;

        
      </description>
    </item>
    
    <item>
      <title>层叠样式表 (Cascading Style Sheets)小结</title>
      <link>/blog/cn/2017/09/css-study/</link>
      <pubDate>Mon, 04 Sep 2017 23:03:13 +0000</pubDate>
      
      <guid>/blog/cn/2017/09/css-study/</guid>
      <description>
        

&lt;p&gt;CSS是一个我一直认为web前段技术，我知道我肯定能学会，但是我却不肯学，可是可是我总是对网页有一种莫名的向往，现在有了R,markdown,shiny,可以回避JSP,PHP等总不愿意学会的工具了，可是HTML,CSS,JS却无法回避了,这是开始shiny之后又不得不继续深入的一个topic. Shiny 的表现太土了.&lt;br /&gt;
  一些概念：样式表定义如何显示 HTML 元素&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;css&#34;&gt;CSS&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.runoob.com/css/css-intro.html&#34;&gt;在线教程&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.runoob.com/cssref/css-reference.html&#34;&gt;参考手册&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;css-语法&#34;&gt;CSS 语法&lt;/h1&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;CSS 规则由两个主要的部分构成：选择器，以及一条或多条声明:&lt;br /&gt;
如：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;p{  
text-align:center;  /*这是另一个注释*/  
color:black;  
font-family:arial;  
} 
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;id 选择器: id 选择器以 &amp;ldquo;#&amp;rdquo; 来定义&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#para1
{
text-align:center;
color:red;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;class 选择器：类选择器以一个点&amp;rdquo;.&amp;ldquo;号显示&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;.cen {text-align:center;}  
/*所以拥有cen类的HTML元素都居中*/  
另外，可以指定所有 p 元素使用 class=&amp;quot;center&amp;quot; 让该元素的文本居中  
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  &amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt; 
&amp;lt;title&amp;gt;菜鸟教程(runoob.com)&amp;lt;/title&amp;gt; 
&amp;lt;style&amp;gt;
p.center
{
    text-align:center;
}
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1 class=&amp;quot;center&amp;quot;&amp;gt;这个标题不受影响&amp;lt;/h1&amp;gt;
&amp;lt;p class=&amp;quot;center&amp;quot;&amp;gt;这个段落居中对齐。&amp;lt;/p&amp;gt; 
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;样式表种类&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;外部样式表&lt;/li&gt;

&lt;li&gt;&lt;p&gt;内部样式表&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;内联样式&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  &amp;lt;head&amp;gt;
    &amp;lt;!-- 外部样式 style.css --&amp;gt;
    &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; type=&amp;quot;text/css&amp;quot; href=&amp;quot;style.css&amp;quot;/&amp;gt;
    &amp;lt;!-- 设置：h3{color:blue;} --&amp;gt;
    &amp;lt;style type=&amp;quot;text/css&amp;quot;&amp;gt;
      /* 内部样式 */
      h3{color:green;}
    &amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
    &amp;lt;h3&amp;gt;测试！&amp;lt;/h3&amp;gt;
&amp;lt;/body&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;CSS 背景&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  背景颜色
  body {background-color:#b0c4de;}   #&amp;quot;#ff0000&amp;quot;,&amp;quot;rgb(255,0,0)&amp;quot;,&amp;quot;red&amp;quot;三种表示都可以
  h1 {background-color:#6495ed;}
  p {background-color:#e0ffff;}
  div {background-color:#b0c4de;}
  背景图像
  body {background-image:url(&#39;paper.gif&#39;);}
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;CSS 文本格式&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;##颜色
body {color:red;}
h1 {color:#00ff00;}
h2 {color:rgb(255,0,0);}
##对齐
h1 {text-align:center;}
p.date {text-align:right;}
p.main {text-align:justify;}
##文本修饰
h1 {text-decoration:overline;}
h2 {text-decoration:line-through;}
h3 {text-decoration:underline;}
## 文本转换
p.uppercase {text-transform:uppercase;}
p.lowercase {text-transform:lowercase;}
p.capitalize {text-transform:capitalize;}
##文本缩进
p {text-indent:50px;}
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;CSS 字体&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;##字体样式
&amp;lt;style&amp;gt;
p.normal {font-style:normal;}
p.italic {font-style:italic;}
p.oblique {font-style:oblique;}
&amp;lt;/style&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;p class=&amp;quot;normal&amp;quot;&amp;gt;这是一个段落,正常。&amp;lt;/p&amp;gt;
&amp;lt;p class=&amp;quot;italic&amp;quot;&amp;gt;这是一个段落,斜体。&amp;lt;/p&amp;gt;
&amp;lt;p class=&amp;quot;oblique&amp;quot;&amp;gt;这是一个段落,斜体。&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;

##字体大小
h1 {font-size:40px;}
h2 {font-size:30px;}
p {font-size:14px;}
h1 {font-size:2.5em;} /* 40px/16=2.5em */
h2 {font-size:1.875em;} /* 30px/16=1.875em */
p {font-size:0.875em;} /* 14px/16=0.875em */
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;学到这里也就了解CSS的一个基本语法结构了，不需要深入了&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Shiny and shinyboard学习</title>
      <link>/blog/cn/2017/08/shiny-and-shinyboard/</link>
      <pubDate>Tue, 29 Aug 2017 16:50:01 +0000</pubDate>
      
      <guid>/blog/cn/2017/08/shiny-and-shinyboard/</guid>
      <description>
        

&lt;p&gt;Rblogdonw+Hugo用来实现静态网页，Shiny用于实现动态网页，几乎把R这个工具发挥到了极致，使其无所不能，R可以解决一切&lt;strong&gt;简单&lt;/strong&gt;的需要了。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;1-shiny&#34;&gt;1. Shiny&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://yanping.me/shiny-tutorial/&#34;&gt;中文教程&lt;/a&gt;. Among them, the Articles are very useful.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://shiny.rstudio.com/tutorial/&#34;&gt;英文教程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rstudio/shiny-examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然而shiny太素颜了，需要一些扩展使得其表现美妙起来。&lt;/p&gt;

&lt;h1 id=&#34;2-shiny-with-html&#34;&gt;2. Shiny with HTML&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;names(shiny::tages)  ## 100多种HTML标签

tags$div(class = &amp;quot;header&amp;quot;, checked = NA,
  tags$p(&amp;quot;Ready to take the Shiny tutorial? If so&amp;quot;),
  tags$a(href = &amp;quot;shiny.rstudio.com/tutorial&amp;quot;, &amp;quot;Click Here!&amp;quot;)
)
## &amp;lt;div class=&amp;quot;header&amp;quot; checked&amp;gt;
##   &amp;lt;p&amp;gt;Ready to take the Shiny tutorial? If so&amp;lt;/p&amp;gt;
##   &amp;lt;a href=&amp;quot;shiny.rstudio.com/tutorial&amp;quot;&amp;gt;Click Here!&amp;lt;/a&amp;gt;
## &amp;lt;/div&amp;gt; 

withTags({
  div(class=&amp;quot;header&amp;quot;, checked=NA,
    p(&amp;quot;Ready to take the Shiny tutorial? If so&amp;quot;),
    a(href=&amp;quot;shiny.rstudio.com/tutorial&amp;quot;, &amp;quot;Click Here!&amp;quot;)
  )
})
## &amp;lt;div class=&amp;quot;header&amp;quot; checked&amp;gt;
##   &amp;lt;p&amp;gt;Ready to take the Shiny tutorial? If so&amp;lt;/p&amp;gt;
##   &amp;lt;a href=&amp;quot;shiny.rstudio.com/tutorial&amp;quot;&amp;gt;Click Here!&amp;lt;/a&amp;gt;
## &amp;lt;/div&amp;gt; 

##lists
tags$div(class=&amp;quot;header&amp;quot;, checked=NA,
  list(
    tags$p(&amp;quot;Ready to take the Shiny tutorial? If so&amp;quot;),
    tags$a(href=&amp;quot;shiny.rstudio.com/tutorial&amp;quot;, &amp;quot;Click Here!&amp;quot;),
    &amp;quot;Thank you&amp;quot;
  )
)
## &amp;lt;div class=&amp;quot;header&amp;quot; checked&amp;gt;
##   &amp;lt;p&amp;gt;Ready to take the Shiny tutorial? If so&amp;lt;/p&amp;gt;
##   &amp;lt;a href=&amp;quot;shiny.rstudio.com/tutorial&amp;quot;&amp;gt;Click Here!&amp;lt;/a&amp;gt;
##   Thank you
## &amp;lt;/div&amp;gt; 


# Raw HTML 加 HTML()
tags$div(
  HTML(&amp;quot;&amp;lt;strong&amp;gt;Raw HTML!&amp;lt;/strong&amp;gt;&amp;quot;)
)
## &amp;lt;div&amp;gt;&amp;lt;strong&amp;gt;Raw HTML!&amp;lt;/strong&amp;gt;&amp;lt;/div&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HTML 前端+ Shiny服务器端数据响应机制&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;HTML form elements (in this case a select list and a number input) are bound to input slots using their &lt;strong&gt;name&lt;/strong&gt; attribute.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Output is rendered into HTML elements based on matching their &lt;strong&gt;id&lt;/strong&gt; attribute to an output slot and by specifying the requisite css class for the element (in this case either shiny-text-output, shiny-plot-output, or shiny-html-output)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;library(shiny)
runExample(&amp;quot;08_html&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;3-shinydashboard&#34;&gt;3. Shinydashboard&lt;/h1&gt;

&lt;p&gt;这是一个扩展R包，使其有一些面板功能。入门讲解在&lt;a href=&#34;https://rstudio.github.io/shinydashboard/index.html&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;4-shinyjs&#34;&gt;4. Shinyjs&lt;/h1&gt;

&lt;p&gt;从&lt;a href=&#34;https://github.com/daattali/shinyjs&#34;&gt;这里学习&lt;/a&gt;，这个放在以后再深入吧。&lt;/p&gt;

&lt;h1 id=&#34;5-shinythemes&#34;&gt;5. Shinythemes&lt;/h1&gt;

&lt;h1 id=&#34;6-shiny-server&#34;&gt;6. shiny server&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/shiny-server&#34;&gt;学习入口&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://www.rstudio.com/products/shiny/download-server/&#34;&gt;最新安装文件&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;$ sudo su - \
-c &amp;quot;R -e \&amp;quot;install.packages(&#39;shiny&#39;, repos=&#39;https://cran.rstudio.com/&#39;)\&amp;quot;&amp;quot;
$ sudo apt-get install gdebi-core
$ wget https://download3.rstudio.org/ubuntu-12.04/x86_64/shiny-server-1.5.4.869-amd64.deb
$ sudo gdebi shiny-server-1.5.4.869-amd64.deb

start shiny-server 　　　　   # 启动
stop shiny-server 　　　　　# 停止
restart shiny-server 　　　　# 重启
status shiny-server 　　　　#查看状态  
reload shiny-server 　　　　#不中断服务的前提下 更新加载配置项

##web 查看
localhost:3838   默认端口时3838，可以在配置文件(/etc/shiny-server/shiny-server.conf)中修改：端口 和run_as默认为shiny,可改为username

Shiny Server默认会在/srv/shiny-server/目录查找你的Apps程序,可以把你开发的apps拷贝这个目录下
sudo cp ~/app file /srv/shiny-server/ 将文件拷入

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;######
一个生信技能树的参考教程
前几天刚好在亚马逊云上注册了一个1年免费的Amazon Web Services (AWS) ，正好以此来尝试学习下shiny的相关东西。

主要参考了：
http://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/
http://www.bio-info-trainee.com/1683.html
操作系统：ubuntu
1. 安装 R
        sudo apt install r-base
2. 安装
Rstudio-server
        sudo apt-get install gdebi-core
        sudo apt-get install libapparmor1
        wget https://download2.rstudio.org/rstudio-server-1.0.143-amd64.deb
        sudo gdebi rstudio-server-1.0.143-amd64.deb
因为Rstudio-server不能以root用户登录，所以我们需要创建一个用户
        sudo adduser xxxxx
        ......
然后在网页上输入ip:8787进入Rstudio-server界面，输入用户和密码，即可登录

3. 安装
Shiny
        sudo su - -c &amp;quot;R -e \&amp;quot;install.packages(&#39;shiny&#39;, repos=&#39;http://cran.rstudio.com/&#39;)\&amp;quot;&amp;quot;
不能直接进入R，然后install.packages(&amp;quot;shiny&amp;quot;)，因为如果这样安装，是将shiny包安装下当前登录用户的个人library中，使得最终shiny-server无法运行
        apt-get install gdebi-core
        wget https://download3.rstudio.org/ub ... 1.5.3.838-amd64.deb
        sudo gdebi shiny-server-1.5.3.838-amd64.deb

做完以上几步后，shiny-server算是初步安装好了，然后可以在网页上ip:3838进入shiny-server界面(ip是你服务器的ip地址)。一般我们能看到左边一列的文字和右边的两个框。当然还需要再安装个rmarkdown，不然还是会有error的

        sudo su - -c &amp;quot;R -e \&amp;quot;install.packages(&#39;rmarkdown&#39;, repos=&#39;http://cran.rstudio.com/&#39;)\&amp;quot;&amp;quot;

4. 配置shiny server

    * Shiny Server log is at /var/log/shiny-server.log
    * The default Shiny Server homepage you’re seeing is located at /srv/shiny-server/index.html
- you can edit it or remove it.
    * Any Shiny app directory that you place under /srv/shiny-server/ will be served as a Shiny app. For example, there is a default app at /srv/shiny-server/sample-apps/hello/, which means you can run the app by going to http://123.456.1.2:3838/sample-apps/hello/

    * The config file for Shiny Server is at /etc/shiny-server/shiny-server.conf
    * To reload the server after editing the config, use sudo reload shiny-server
    * When hosting an Rmarkdown file, name the file index.rmd and add runtime: shiny to the document’s frontmatter

5. 赋予shiny权限
假设当你登录是以自己用户登录，你在shiny server创建的文件只有该用户（除了root）才有权限读写，但是shiny server是以shiny用户来运行shiny的app，所以要给予shiny用户在一些目录的权限。
例如：

        sudo groupadd shiny-apps
        sudo usermod -aG shiny-apps dean
        sudo usermod -aG shiny-apps shiny
        sudo chown -R dean:shiny-apps /srv/shiny-server
        sudo chmod g+w /srv/shiny-server
        sudo chmod g+s /srv/shiny-server  ####在该目录下创建的文件都属于该目录所属的组
        
6. 下载shiny官网的例子
git clone https://github.com/rstudio/shiny-examples.git
文件是要下载到/srv/shiny-server中
7. 运行例子程序
http://ip:3838/shiny-examples/010-download/ （ip是你服务器的ip地址）
想要运行哪个shiny app，只要在ip:3838/后面添加/srv/shiny-server中的文件的相对路径即可
###

&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;最近用shiny做了一个页面，也是一个艰难的开始。Mark in(&amp;ldquo;Mon Sep 04 19:03:13 2017&amp;rdquo;)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>常用命令集合</title>
      <link>/blog/cn/2017/08/a-collection-of-command/</link>
      <pubDate>Mon, 28 Aug 2017 12:00:58 +0000</pubDate>
      
      <guid>/blog/cn/2017/08/a-collection-of-command/</guid>
      <description>
        

&lt;h1 id=&#34;dos&#34;&gt;DOS&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;cd /   [root]&lt;/li&gt;
&lt;li&gt;cd ~  [home or root/user]&lt;/li&gt;
&lt;li&gt;cd . [Current]&lt;/li&gt;
&lt;li&gt;cd .. [Father]&lt;/li&gt;
&lt;li&gt;cd - [previous]&lt;/li&gt;
&lt;li&gt;pwd [current]&lt;/li&gt;
&lt;li&gt;d: +Enter [D:/]&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;ubuntu&#34;&gt;Ubuntu&lt;/h1&gt;

&lt;h3 id=&#34;basic&#34;&gt;Basic&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;sudo su 进入root&lt;/li&gt;
&lt;li&gt;exit &lt;strong&gt;or&lt;/strong&gt; logout  &lt;strong&gt;or&lt;/strong&gt; ctrl D 退出root&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;download&#34;&gt;Download&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&amp;amp; 加在一个命令之后可以把这个命令放到后台运行&lt;/li&gt;
&lt;li&gt;Ctrl+Z 可以把一个前台执行的命令放到后台&lt;/li&gt;
&lt;li&gt;jobs -l 查看后台运行的命令&lt;/li&gt;
&lt;li&gt;fg %jobnumber 将后台命令调到前台&lt;/li&gt;
&lt;li&gt;bg %jobnumber 重启后台暂停的命令&lt;/li&gt;
&lt;li&gt;kill %num 杀死&lt;/li&gt;
&lt;li&gt;ps 查看进程号PID&lt;/li&gt;
&lt;li&gt;nohup 始终执行&lt;/li&gt;
&lt;li&gt;ps - aux | grep xxx 显示终端关闭后的后台进程


&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;修改权限和所有者&#34;&gt;修改权限和所有者&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;[chmod -R 700 Document/]变更权限(-R参数是递归)&lt;/li&gt;
&lt;li&gt;[chown -R username:root Document/]修改所有者为root用户组的username用户&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&#34;常用几种&#34;&gt;常用几种&lt;/h6&gt;

&lt;ul&gt;
&lt;li&gt;sudo chmod -[读,写,执行]×××（所有者）×××（组用户）×××（其他用户）&lt;/li&gt;
&lt;li&gt;sudo chmod 600 ××× （只有所有者有读和写的权限）&lt;/li&gt;
&lt;li&gt;sudo chmod 644 ××× （所有者有读和写的权限，组用户只有读的权限）&lt;/li&gt;
&lt;li&gt;sudo chmod 700 ××× （只有所有者有读和写以及执行的权限）&lt;/li&gt;
&lt;li&gt;sudo chmod 666 ××× （每个人都有读和写的权限）&lt;/li&gt;
&lt;li&gt;sudo chmod 777 ××× （每个人都有读和写以及执行的权限）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;磁盘管理&#34;&gt;磁盘管理&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;mkdir -p 递归创建目录&lt;/li&gt;
&lt;li&gt;cp 复制&lt;/li&gt;
&lt;li&gt;rmdir 删除空目录&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;df -lh 查看磁盘空间&lt;br /&gt;
df -kh 查看磁盘挂载&lt;/li&gt;
&lt;li&gt;sudo fdisk -lu 显示硬盘及所属分区情况&lt;/li&gt;
&lt;li&gt;sudo fdisk /dev/sdb 对sbd盘分区&lt;br /&gt;
m(help)-n(增加一个新分区)-e(扩展分区)-1-4(分几个区)-&amp;hellip;-p(显示分区表)-w(保存分区表)&lt;/li&gt;
&lt;li&gt;sudo mkfs -t ext4 /dev/sdb 对sdb盘格式化为ext4格式&lt;/li&gt;
&lt;li&gt;sudo blkid 查看分区的UUID&lt;/li&gt;
&lt;li&gt;sudo blkid /dev/sda5 查看指定盘的UUID&lt;/li&gt;
&lt;li&gt;sudo mount -t ext4 /dev/sdb /data1 挂载到data1&lt;/li&gt;

&lt;li&gt;&lt;p&gt;suod umount /dev/sdb 卸载&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;sudo chown -R username:root Document/  修改文件夹所有者&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ubuntu 加载新硬盘教程&lt;a href=&#34;http://note.youdao.com/noteshare?id=19c72003e1f78ed8dafbe12c53c6e150&amp;amp;sub=8E9A1BEDF6AE4F73AB50892F428B249C&#34;&gt;Here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ubuntu&lt;a href=&#34;https://taoshengxu.github.io/DocumentGit/pdf/Ubuntu命令手册.pdf&#34;&gt;命令手册&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;vi-操作&#34;&gt;vi 操作&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;基本上vi可以分为三种状态

&lt;ul&gt;
&lt;li&gt;分别是命令模式（command mode）：移动光标,删除&lt;/li&gt;
&lt;li&gt;插入模式（Insert mode）：用于输入字符&lt;/li&gt;
&lt;li&gt;底行模式（last line mode）：设置编辑环境和保存&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;进入vi  初始为命令模式
$vi file&lt;/li&gt;
&lt;li&gt;进入插入模式： 在命令模式下输入[i]&lt;/li&gt;
&lt;li&gt;ESC 返回命令模式&lt;/li&gt;
&lt;li&gt;[:]进入底行模式

&lt;ul&gt;
&lt;li&gt;[w fileName]以指定名字保存&lt;/li&gt;
&lt;li&gt;[wq]存盘并退出&lt;/li&gt;
&lt;li&gt;[q!]不存盘退出&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cnblogs.com/itech/archive/2009/04/17/1438439.html&#34;&gt;更复杂用法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

        
      </description>
    </item>
    
    <item>
      <title>上线测试</title>
      <link>/blog/cn/2017/08/sem3/</link>
      <pubDate>Sun, 27 Aug 2017 20:56:14 +0000</pubDate>
      
      <guid>/blog/cn/2017/08/sem3/</guid>
      <description>
        &lt;p&gt;今天终于把blog搭建好了，mark and test&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>一些关于基因组名字的含义</title>
      <link>/blog/cn/2017/07/sem2/</link>
      <pubDate>Tue, 18 Jul 2017 14:18:14 +0000</pubDate>
      
      <guid>/blog/cn/2017/07/sem2/</guid>
      <description>
        &lt;p&gt;NC表示人类基因组DNA的RefSeq。&lt;/p&gt;

&lt;p&gt;NM表示mRNA的RefSeq。&lt;/p&gt;

&lt;p&gt;NP表示蛋白质的RefSeq。&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>序言</title>
      <link>/blog/cn/2017/07/preface/</link>
      <pubDate>Sun, 16 Jul 2017 14:18:14 +0000</pubDate>
      
      <guid>/blog/cn/2017/07/preface/</guid>
      <description>
        

&lt;h1 id=&#34;无一事马虎-无一日懈怠&#34;&gt;无一事马虎 无一日懈怠&lt;/h1&gt;

&lt;p&gt;好句，若真如此，生命无光，况凡人不可为之。可作凡人的目标，努力逼近它吧。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;最近几年我总是在寻找一个网络服务系统，能使我平生所得所获的有一个的去处，方便以后某个时间点来寻他们。在IT的世界里，我一直认为重要的是一种知识概念和印象，以前学过做过，过一段时间还是得从头再来，再做一遍与第一遍的时间相差无几， 假如你没有保存的话&amp;hellip;..&lt;/p&gt;

&lt;p&gt;Hogo的出现使最熟练R的人感到喜悦。看到那么多Hugo Themes，于是我开始挑啊，总告诉我有适合我的，却总是找不到它。我想找的它是一个目录列表+时间，目录列表的字体不要太大，这样能方便我快速的定位和找回过去的记忆，也省去频繁的翻页。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://yihui.name/&#34;&gt;Yihui&lt;/a&gt; 是我在打开Bioinformatics世界之初就偶然关注到的一个[学者]  OR  [IT工程师]  OR   [老师]，偶然和必然等意，Bioinformatics and Statistics的天下谁人不识君。他的页面和他的博客样式是我梦寐以求，就是它了。我没办法自己修改主题，我也不屑再去学，睡觉的时间宝贵，不要总是学，什么都学，学来学去魂消了。&lt;/p&gt;

&lt;p&gt;Yihui 吟的几句词也还不失雅趣，不舍删留此处，可算小读怡情。以后的拾遗也填在这里吧&amp;hellip;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;深情似海，问相逢初度，是何年纪？依约而今还记取，不是前生夙世。放学花前，题诗石上，春水园亭里。逢君一笑，人间无此欢喜。&lt;br /&gt;
无奈苍狗看云，红羊数劫，惘惘休提起。客气渐多真气少，汩没心灵何已。千古声名，百年担负，事事违初意。心头阁住，儿时那种情味。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;END&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
